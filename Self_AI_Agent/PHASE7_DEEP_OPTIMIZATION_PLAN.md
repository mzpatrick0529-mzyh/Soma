# ğŸš€ PHASE 7: æ·±åº¦ä¼˜åŒ–å®æ–½è®¡åˆ’
## ä»åŸå‹åˆ°çœŸæ­£çš„ç”Ÿäº§çº§AIç³»ç»Ÿ

**åˆ¶å®šæ—¥æœŸ**: 2025-10-24  
**é¢„è®¡å·¥æœŸ**: 8-10å‘¨  
**ç›®æ ‡**: å°†å½“å‰61%ç”Ÿäº§å°±ç»ªåº¦æå‡åˆ°90%+

---

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

åŸºäºæ·±åº¦ä»£ç å®¡æŸ¥ï¼ŒPhase 0-6å­˜åœ¨**36ä¸ªå…³é”®ç¼ºé™·**ã€‚Phase 7å°†é€šè¿‡3ä¸ªå­é˜¶æ®µç³»ç»Ÿæ€§ä¿®å¤:

1. **Phase 7A: æ ¸å¿ƒMLæ¨¡å‹å‡çº§** (3-4å‘¨) - æ›¿æ¢è§„åˆ™ä¸ºçœŸå®æ·±åº¦å­¦ä¹ 
2. **Phase 7B: ç”Ÿäº§åŸºç¡€è®¾æ–½å®Œå–„** (2-3å‘¨) - è¡¥é½ç¼ºå¤±çš„ç”Ÿäº§ç»„ä»¶
3. **Phase 7C: æ€§èƒ½ä¼˜åŒ–ä¸éªŒè¯** (2-3å‘¨) - çœŸæ­£çš„é‡åŒ–/å‰ªæ/è’¸é¦

---

## ğŸ¯ Phase 7A: æ ¸å¿ƒMLæ¨¡å‹å‡çº§ (3-4å‘¨)

### Week 1-2: æƒ…æ„Ÿä¸æ¨ç†æ¨¡å‹é›†æˆ

#### Task 7A.1: æ·±åº¦æƒ…æ„Ÿè¯†åˆ«æ¨¡å‹

**å½“å‰é—®é¢˜**: ä»…å…³é”®è¯åŒ¹é…ï¼Œå‡†ç¡®ç‡~45%  
**ç›®æ ‡**: å‡†ç¡®ç‡æå‡åˆ°85%+ï¼Œæ”¯æŒéšå«æƒ…æ„Ÿã€è®½åˆºã€æƒ…æ„Ÿæ··åˆ

**å®æ–½æ­¥éª¤**:

```bash
# 1. å®‰è£…ä¾èµ–
pip install transformers torch sentencepiece
pip install emoji demoji  # å¤„ç†emojiæƒ…æ„Ÿ

# 2. ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹
python -c "
from transformers import pipeline
model = pipeline('text-classification', 
                 model='SamLowe/roberta-base-go_emotions',
                 device=0)
print('GoEmotionsæ¨¡å‹ä¸‹è½½å®Œæˆ')
"

# 3. åˆ›å»ºæ–°çš„emotionæ¨¡å‹
```

**ä»£ç æ–‡ä»¶**: `src/ml/services/emotion_model_v2.py`

```python
"""
Advanced Emotion Recognition using Transformers
"""
from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    pipeline
)
import torch
import numpy as np
from typing import Dict, List, Optional
import emoji

class TransformerEmotionAnalyzer:
    """
    åŸºäºTransformerçš„æ·±åº¦æƒ…æ„Ÿåˆ†æ
    
    æ¨¡å‹:
    - GoEmotions (28 emotions)
    - Sarcasm detector
    - Emotional intensity regressor
    """
    
    def __init__(self, device: str = "cuda" if torch.cuda.is_available() else "cpu"):
        # ä¸»æƒ…æ„Ÿåˆ†ç±»å™¨ (28ç§æƒ…æ„Ÿ)
        self.emotion_classifier = pipeline(
            "text-classification",
            model="SamLowe/roberta-base-go_emotions",
            top_k=None,
            device=0 if device == "cuda" else -1
        )
        
        # è®½åˆºæ£€æµ‹å™¨
        self.sarcasm_detector = pipeline(
            "text-classification",
            model="mrm8488/t5-base-finetuned-sarcasm-twitter",
            device=0 if device == "cuda" else -1
        )
        
        # æƒ…æ„Ÿå¼ºåº¦å›å½’å™¨
        self.intensity_model = AutoModelForSequenceClassification.from_pretrained(
            "cardiffnlp/twitter-roberta-base-emotion-multilabel-latest"
        ).to(device)
        self.intensity_tokenizer = AutoTokenizer.from_pretrained(
            "cardiffnlp/twitter-roberta-base-emotion-multilabel-latest"
        )
        
        self.device = device
        
        # æƒ…æ„Ÿæ˜ å°„åˆ°Plutchikæ¨¡å‹
        self.emotion_mapping = self._build_emotion_mapping()
    
    def analyze(
        self,
        text: str,
        context: Optional[List[str]] = None
    ) -> Dict[str, any]:
        """
        å®Œæ•´çš„æƒ…æ„Ÿåˆ†æ
        
        Returns:
            - primary_emotion: ä¸»è¦æƒ…æ„Ÿ
            - emotion_scores: æ‰€æœ‰æƒ…æ„Ÿå¾—åˆ†
            - intensity: æƒ…æ„Ÿå¼ºåº¦ (0-1)
            - valence: æƒ…æ„Ÿæ•ˆä»· (-1åˆ°1)
            - arousal: å”¤é†’åº¦ (0-1)
            - is_sarcastic: æ˜¯å¦è®½åˆº
            - mixed_emotions: æ··åˆæƒ…æ„Ÿ
        """
        # 1. åŸºç¡€æƒ…æ„Ÿåˆ†ç±»
        emotions = self.emotion_classifier(text)[0]
        
        # 2. è®½åˆºæ£€æµ‹
        sarcasm = self.sarcasm_detector(text)[0]
        is_sarcastic = sarcasm['label'] == 'LABEL_1' and sarcasm['score'] > 0.7
        
        # 3. å¼ºåº¦ä¼°è®¡
        intensity = self._estimate_intensity(text, emotions)
        
        # 4. Valenceå’ŒArousalè®¡ç®—
        valence, arousal = self._calculate_va(emotions)
        
        # 5. æ··åˆæƒ…æ„Ÿæ£€æµ‹
        mixed = self._detect_mixed_emotions(emotions)
        
        # 6. ä¸Šä¸‹æ–‡è°ƒæ•´
        if context:
            emotions = self._adjust_for_context(emotions, context)
        
        # 7. å¦‚æœæ˜¯è®½åˆºï¼Œåè½¬æƒ…æ„Ÿ
        if is_sarcastic:
            emotions = self._invert_emotions(emotions)
        
        # æ’åº
        sorted_emotions = sorted(emotions, key=lambda x: x['score'], reverse=True)
        
        return {
            'primary_emotion': sorted_emotions[0]['label'],
            'primary_score': sorted_emotions[0]['score'],
            'all_emotions': {e['label']: e['score'] for e in emotions},
            'intensity': intensity,
            'valence': valence,
            'arousal': arousal,
            'is_sarcastic': is_sarcastic,
            'mixed_emotions': mixed,
            'confidence': self._calculate_confidence(sorted_emotions)
        }
    
    def _estimate_intensity(
        self,
        text: str,
        emotions: List[Dict]
    ) -> float:
        """
        ä¼°è®¡æƒ…æ„Ÿå¼ºåº¦
        
        è€ƒè™‘å› ç´ :
        - å¤§å†™å­—æ¯ (SHOUTING!)
        - æ ‡ç‚¹ç¬¦å· (!!!, ???)
        - Emoji
        - å¼ºåŒ–è¯ (very, extremely, so)
        """
        intensity = 0.5  # åŸºå‡†
        
        # å¤§å†™å­—æ¯æ¯”ä¾‹
        upper_ratio = sum(c.isupper() for c in text) / max(len(text), 1)
        intensity += min(upper_ratio * 2, 0.3)
        
        # æ„Ÿå¹å·/é—®å·
        exclamations = text.count('!') + text.count('?')
        intensity += min(exclamations * 0.1, 0.3)
        
        # Emojiæƒ…æ„Ÿå¼ºåº¦
        emojis = emoji.emoji_list(text)
        if emojis:
            intensity += min(len(emojis) * 0.05, 0.2)
        
        # å¼ºåŒ–è¯
        intensifiers = ['very', 'extremely', 'so', 'really', 'absolutely', 'totally']
        for word in intensifiers:
            if word in text.lower():
                intensity += 0.1
        
        return min(intensity, 1.0)
    
    def _calculate_va(
        self,
        emotions: List[Dict]
    ) -> tuple[float, float]:
        """
        è®¡ç®—Valence (æ•ˆä»·) å’Œ Arousal (å”¤é†’åº¦)
        
        åŸºäºæƒ…æ„Ÿçš„VAåæ ‡
        """
        # Emotion â†’ VAæ˜ å°„ (åŸºäºå¿ƒç†å­¦ç ”ç©¶)
        va_mapping = {
            'joy': (0.8, 0.7),
            'excitement': (0.9, 0.9),
            'love': (0.9, 0.6),
            'surprise': (0.3, 0.8),
            'sadness': (-0.7, 0.3),
            'anger': (-0.8, 0.8),
            'fear': (-0.7, 0.9),
            'disgust': (-0.8, 0.5),
            'neutral': (0.0, 0.2),
            # ... æ·»åŠ æ‰€æœ‰28ç§æƒ…æ„Ÿ
        }
        
        valence = 0.0
        arousal = 0.0
        
        for emotion in emotions:
            label = emotion['label']
            score = emotion['score']
            
            if label in va_mapping:
                v, a = va_mapping[label]
                valence += v * score
                arousal += a * score
        
        return valence, arousal
    
    def _detect_mixed_emotions(
        self,
        emotions: List[Dict]
    ) -> Optional[str]:
        """
        æ£€æµ‹æ··åˆæƒ…æ„Ÿ (å¦‚å–œå¿§å‚åŠ)
        """
        sorted_emotions = sorted(emotions, key=lambda x: x['score'], reverse=True)
        
        # æ£€æŸ¥top 2æƒ…æ„Ÿ
        if len(sorted_emotions) >= 2:
            first = sorted_emotions[0]
            second = sorted_emotions[1]
            
            # å¦‚æœä¸¤è€…å¾—åˆ†æ¥è¿‘ä¸”æ•ˆä»·ç›¸å
            if (second['score'] > 0.3 and 
                abs(first['score'] - second['score']) < 0.3):
                
                # æ£€æŸ¥æ•ˆä»·
                v1, a1 = self._get_va(first['label'])
                v2, a2 = self._get_va(second['label'])
                
                if v1 * v2 < 0:  # ç›¸åæ•ˆä»·
                    return f"mixed_{first['label']}_{second['label']}"
        
        return None
    
    def build_emotional_trajectory(
        self,
        user_id: str,
        days: int = 30
    ) -> Dict[str, any]:
        """
        æ„å»ºç”¨æˆ·æƒ…æ„Ÿè½¨è¿¹
        
        ç”¨äº:
        1. æ£€æµ‹å¿ƒç†å¥åº·è¶‹åŠ¿
        2. ç†è§£æƒ…æ„ŸåŸºçº¿
        3. é¢„æµ‹æƒ…æ„Ÿååº”
        """
        from .db_utils import db
        
        conversations = db.get_user_conversations(user_id, days=days)
        
        trajectory = []
        for conv in conversations:
            if conv['role'] == 'user':
                analysis = self.analyze(conv['content'])
                trajectory.append({
                    'timestamp': conv['timestamp'],
                    'valence': analysis['valence'],
                    'arousal': analysis['arousal'],
                    'primary_emotion': analysis['primary_emotion']
                })
        
        # ç»Ÿè®¡åˆ†æ
        valences = [t['valence'] for t in trajectory]
        arousals = [t['arousal'] for t in trajectory]
        
        # è¶‹åŠ¿æ£€æµ‹
        if len(valences) >= 7:
            trend_slope = np.polyfit(range(len(valences)), valences, 1)[0]
        else:
            trend_slope = 0
        
        return {
            'trajectory': trajectory,
            'baseline_valence': np.mean(valences),
            'valence_std': np.std(valences),
            'baseline_arousal': np.mean(arousals),
            'trend': 'improving' if trend_slope > 0.01 else 
                     'declining' if trend_slope < -0.01 else 'stable',
            'volatility': np.std(valences),
            'dominant_emotions': self._get_dominant_emotions(trajectory)
        }
```

**æµ‹è¯•è®¡åˆ’**:
```python
# tests/test_emotion_v2.py
def test_emotion_accuracy():
    analyzer = TransformerEmotionAnalyzer()
    
    test_cases = [
        ("I'm so happy!", "joy", 0.8),
        ("This is terrible.", "anger", 0.7),
        ("Oh great, another problem.", "sarcasm+frustration", 0.6),
    ]
    
    for text, expected, min_score in test_cases:
        result = analyzer.analyze(text)
        assert result['primary_emotion'] in expected
        assert result['primary_score'] >= min_score
```

**é¢„æœŸæå‡**:
- å‡†ç¡®ç‡: 45% â†’ **87%** (+42%)
- æ”¯æŒè®½åˆº: âŒ â†’ âœ…
- æ”¯æŒæ··åˆæƒ…æ„Ÿ: âŒ â†’ âœ…
- æ¨ç†æ—¶é—´: <1ms â†’ 50ms (å¯æ¥å—)

---

#### Task 7A.2: å› æœæ¨ç†æ¨¡å‹é›†æˆ

**å®æ–½æ­¥éª¤**:

```bash
# 1. å®‰è£…å› æœæ¨ç†åº“
pip install dowhy causalml causalnex
pip install networkx==3.1  # å›¾åˆ†æ

# 2. å‡†å¤‡å› æœæ•°æ®é›†
```

**ä»£ç æ–‡ä»¶**: `src/ml/services/causal_reasoner.py`

```python
"""
Causal Reasoning Engine using Causal Discovery
"""
import dowhy
from dowhy import CausalModel
import networkx as nx
import pandas as pd
from typing import Dict, List, Tuple, Optional
from causalnex.structure import StructureModel
from causalnex.structure.notears import from_pandas

class CausalReasoningEngine:
    """
    åŸºäºå› æœå›¾çš„æ¨ç†å¼•æ“
    
    åŠŸèƒ½:
    1. å› æœç»“æ„å‘ç° (NOTEARSç®—æ³•)
    2. å› æœæ•ˆåº”ä¼°è®¡ (DoWhy)
    3. åäº‹å®æ¨ç†
    4. ä¼ é€’æ€§æ¨ç†
    """
    
    def __init__(self):
        self.causal_graphs = {}  # user_id â†’ CausalGraph
        self.causal_models = {}
    
    def discover_causal_structure(
        self,
        user_id: str,
        events: List[Dict[str, any]]
    ) -> StructureModel:
        """
        ä»äº‹ä»¶åºåˆ—ä¸­å‘ç°å› æœç»“æ„
        
        ä½¿ç”¨NOTEARSç®—æ³• (æ— å‘â†’æœ‰å‘å›¾)
        """
        # 1. è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(events)
        
        # 2. åº”ç”¨NOTEARSç®—æ³•
        sm = from_pandas(
            df,
            tabu_edges=self._get_temporal_constraints(events),
            w_threshold=0.3  # è¾¹æƒé‡é˜ˆå€¼
        )
        
        # 3. ä¿å­˜
        self.causal_graphs[user_id] = sm
        
        return sm
    
    def estimate_causal_effect(
        self,
        user_id: str,
        treatment: str,
        outcome: str,
        confounders: List[str] = None
    ) -> Dict[str, float]:
        """
        ä¼°è®¡å› æœæ•ˆåº”
        
        Example:
        estimate_causal_effect(
            treatment="work_overtime",
            outcome="stress_level",
            confounders=["deadline_pressure", "workload"]
        )
        
        Returns:
            - ate: Average Treatment Effect
            - confidence_interval: (lower, upper)
            - p_value: ç»Ÿè®¡æ˜¾è‘—æ€§
        """
        # è·å–æ•°æ®
        data = self._get_causal_data(user_id)
        
        # æ„å»ºå› æœæ¨¡å‹
        model = CausalModel(
            data=data,
            treatment=treatment,
            outcome=outcome,
            common_causes=confounders or []
        )
        
        # è¯†åˆ«å› æœæ•ˆåº”
        identified_estimand = model.identify_effect(
            proceed_when_unidentifiable=True
        )
        
        # ä¼°è®¡
        estimate = model.estimate_effect(
            identified_estimand,
            method_name="backdoor.propensity_score_matching",
            confidence_intervals=True
        )
        
        # é²æ£’æ€§æ£€éªŒ
        refutation = model.refute_estimate(
            identified_estimand,
            estimate,
            method_name="random_common_cause"
        )
        
        return {
            'ate': estimate.value,
            'confidence_interval': (
                estimate.get_confidence_intervals()[0],
                estimate.get_confidence_intervals()[1]
            ),
            'p_value': refutation.refutation_result['p_value'],
            'is_significant': refutation.refutation_result['p_value'] < 0.05
        }
    
    def counterfactual_reasoning(
        self,
        user_id: str,
        factual: Dict[str, any],
        intervention: Dict[str, any]
    ) -> Dict[str, any]:
        """
        åäº‹å®æ¨ç†
        
        Example:
        factual = {"accepted_offer": True, "satisfaction": 0.6}
        intervention = {"accepted_offer": False}
        
        â†’ "å¦‚æœæ²¡æ¥å—offerï¼Œæ»¡æ„åº¦ä¼šæ˜¯å¤šå°‘ï¼Ÿ"
        """
        causal_graph = self.causal_graphs[user_id]
        
        # ä½¿ç”¨Pearlçš„Do-Calculus
        # P(Y | do(X=x')) vs P(Y | X=x)
        
        # ç®€åŒ–å®ç° (å®Œæ•´ç‰ˆéœ€è¦SCM)
        outcome_node = self._get_outcome_node(factual)
        
        # æ¨¡æ‹Ÿå¹²é¢„
        counterfactual_outcome = self._simulate_intervention(
            causal_graph,
            factual,
            intervention
        )
        
        return {
            'factual_outcome': factual[outcome_node],
            'counterfactual_outcome': counterfactual_outcome,
            'difference': counterfactual_outcome - factual[outcome_node]
        }
    
    def explain_reasoning_chain(
        self,
        user_id: str,
        start_event: str,
        end_event: str
    ) -> List[List[str]]:
        """
        è§£é‡Šæ¨ç†é“¾ (Aå¯¼è‡´Bï¼ŒBå¯¼è‡´C...)
        
        è¿”å›æ‰€æœ‰å¯èƒ½çš„å› æœè·¯å¾„
        """
        graph = self.causal_graphs[user_id]
        
        # æŸ¥æ‰¾æ‰€æœ‰è·¯å¾„
        try:
            paths = list(nx.all_simple_paths(
                graph,
                source=start_event,
                target=end_event,
                cutoff=5  # æœ€å¤§é•¿åº¦
            ))
        except nx.NetworkXNoPath:
            return []
        
        # ä¸ºæ¯æ¡è·¯å¾„è®¡ç®—å¼ºåº¦
        path_strengths = []
        for path in paths:
            strength = 1.0
            for i in range(len(path) - 1):
                edge_weight = graph[path[i]][path[i+1]].get('weight', 0.5)
                strength *= edge_weight
            
            path_strengths.append((path, strength))
        
        # æ’åº
        path_strengths.sort(key=lambda x: x[1], reverse=True)
        
        return path_strengths
```

**é¢„æœŸæå‡**:
- å› æœè¯†åˆ«: 30% â†’ **80%** (+50%)
- æ”¯æŒåäº‹å®: âŒ â†’ âœ…
- æ”¯æŒä¼ é€’æ¨ç†: âŒ â†’ âœ…

---

### Week 3-4: Knowledge Graphä¸å†³ç­–æ¨¡å‹

#### Task 7A.3: Neo4j Knowledge Graphé›†æˆ

**åŸºç¡€è®¾æ–½**:

```yaml
# docker-compose-ml.yml æ·»åŠ Neo4jæœåŠ¡
  neo4j:
    image: neo4j:5-community
    container_name: soma-neo4j
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    environment:
      - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD}
      - NEO4J_dbms_memory_heap_max__size=2G
    volumes:
      - neo4j-data:/data
    networks:
      - soma-network
    restart: unless-stopped
```

**ä»£ç å®ç°**: `src/ml/services/knowledge_graph.py`

```python
"""
Production Knowledge Graph using Neo4j
"""
from neo4j import GraphDatabase
import networkx as nx
from typing import List, Dict, Tuple

class CognitiveKnowledgeGraph:
    """
    ç”¨æˆ·è®¤çŸ¥çŸ¥è¯†å›¾è°±
    
    èŠ‚ç‚¹ç±»å‹:
    - Concept: æ¦‚å¿µ
    - Person: äººç‰©
    - Event: äº‹ä»¶
    - Value: ä»·å€¼è§‚
    
    è¾¹ç±»å‹:
    - CAUSES: å› æœå…³ç³»
    - BELIEVES: ä¿¡å¿µ
    - VALUES: é‡è§†
    - ASSOCIATES: å…³è”
    """
    
    def __init__(self, uri: str, auth: Tuple[str, str]):
        self.driver = GraphDatabase.driver(uri, auth=auth)
    
    def add_concept(
        self,
        user_id: str,
        concept_name: str,
        properties: Dict[str, any]
    ):
        """æ·»åŠ æ¦‚å¿µèŠ‚ç‚¹"""
        with self.driver.session() as session:
            session.run(
                """
                MERGE (u:User {id: $user_id})
                MERGE (c:Concept {name: $concept, user_id: $user_id})
                SET c += $properties
                SET c.updated_at = datetime()
                MERGE (u)-[:HAS_CONCEPT]->(c)
                """,
                user_id=user_id,
                concept=concept_name,
                properties=properties
            )
    
    def add_causal_relation(
        self,
        user_id: str,
        cause: str,
        effect: str,
        strength: float,
        evidence: List[str]
    ):
        """æ·»åŠ å› æœè¾¹"""
        with self.driver.session() as session:
            session.run(
                """
                MATCH (u:User {id: $user_id})
                MATCH (a:Concept {name: $cause, user_id: $user_id})
                MATCH (b:Concept {name: $effect, user_id: $user_id})
                MERGE (a)-[r:CAUSES]->(b)
                SET r.strength = $strength
                SET r.evidence_count = size($evidence)
                SET r.last_observed = datetime()
                """,
                user_id=user_id,
                cause=cause,
                effect=effect,
                strength=strength,
                evidence=evidence
            )
    
    def find_causal_paths(
        self,
        user_id: str,
        start: str,
        end: str,
        max_length: int = 5
    ) -> List[Tuple[List[str], float]]:
        """æŸ¥æ‰¾å› æœè·¯å¾„"""
        with self.driver.session() as session:
            result = session.run(
                """
                MATCH path = (a:Concept {name: $start, user_id: $user_id})
                             -[:CAUSES*1..%d]->(b:Concept {name: $end, user_id: $user_id})
                RETURN [node in nodes(path) | node.name] as path,
                       reduce(s = 1.0, r in relationships(path) | s * r.strength) as strength
                ORDER BY strength DESC
                LIMIT 10
                """ % max_length,
                user_id=user_id,
                start=start,
                end=end
            )
            
            return [(record['path'], record['strength']) for record in result]
    
    def detect_communities(
        self,
        user_id: str
    ) -> Dict[str, List[str]]:
        """æ£€æµ‹è®¤çŸ¥ç¤¾åŒº (æ¦‚å¿µèšç±»)"""
        with self.driver.session() as session:
            # ä½¿ç”¨Louvainç®—æ³•
            session.run(
                """
                CALL gds.louvain.write({
                    nodeProjection: {
                        Concept: {
                            label: 'Concept',
                            properties: ['user_id']
                        }
                    },
                    relationshipProjection: {
                        CAUSES: {type: 'CAUSES', orientation: 'UNDIRECTED'},
                        ASSOCIATES: {type: 'ASSOCIATES', orientation: 'UNDIRECTED'}
                    },
                    writeProperty: 'community'
                })
                """
            )
            
            # è·å–ç¤¾åŒº
            result = session.run(
                """
                MATCH (c:Concept {user_id: $user_id})
                RETURN c.community as community_id, collect(c.name) as concepts
                """,
                user_id=user_id
            )
            
            communities = {}
            for record in result:
                comm_id = f"community_{record['community_id']}"
                communities[comm_id] = record['concepts']
            
            return communities
```

---

## ğŸ¯ Phase 7B: ç”Ÿäº§åŸºç¡€è®¾æ–½å®Œå–„ (2-3å‘¨)

### Week 5-6: æ ¸å¿ƒç”Ÿäº§ç»„ä»¶

#### Task 7B.1: çœŸå®çš„æ¨¡å‹é‡åŒ–/å‰ªæ/è’¸é¦

**ä»£ç æ–‡ä»¶**: `src/ml/optimization/model_quantization.py`

```python
"""
True Model Optimization (Not fake deduplication!)
"""
import torch
import torch.quantization as quantization
from transformers import AutoModel, AutoTokenizer

class ProductionModelOptimizer:
    """
    ç”Ÿäº§çº§æ¨¡å‹ä¼˜åŒ–
    
    æŠ€æœ¯:
    1. åŠ¨æ€é‡åŒ– (FP32â†’INT8)
    2. é™æ€é‡åŒ– (å¸¦æ ¡å‡†)
    3. ç»“æ„åŒ–å‰ªæ (æ•´ä¸ªç¥ç»å…ƒ)
    4. çŸ¥è¯†è’¸é¦ (Teacherâ†’Student)
    """
    
    def quantize_dynamic(
        self,
        model: torch.nn.Module
    ) -> torch.nn.Module:
        """
        åŠ¨æ€é‡åŒ– (æœ€ç®€å•ï¼Œæ— éœ€æ ¡å‡†æ•°æ®)
        
        é€‚ç”¨äº: æ¨ç†å»¶è¿Ÿæ•æ„Ÿå‹åº”ç”¨
        ç²¾åº¦æŸå¤±: ~1-2%
        é€Ÿåº¦æå‡: 2-3x
        å†…å­˜å‡å°‘: 50%
        """
        quantized = torch.quantization.quantize_dynamic(
            model,
            {torch.nn.Linear, torch.nn.LSTM, torch.nn.GRU},
            dtype=torch.qint8
        )
        
        logger.info(f"Dynamic quantization complete")
        return quantized
    
    def quantize_static(
        self,
        model: torch.nn.Module,
        calibration_dataloader: torch.utils.data.DataLoader
    ) -> torch.nn.Module:
        """
        é™æ€é‡åŒ– (æ›´é«˜ç²¾åº¦ï¼Œéœ€è¦æ ¡å‡†æ•°æ®)
        
        ç²¾åº¦æŸå¤±: <1%
        é€Ÿåº¦æå‡: 3-4x
        å†…å­˜å‡å°‘: 75%
        """
        model.eval()
        model.qconfig = quantization.get_default_qconfig('fbgemm')
        
        # èåˆå±‚
        model = quantization.fuse_modules(model, [['conv', 'bn', 'relu']])
        
        # å‡†å¤‡
        model_prepared = quantization.prepare(model)
        
        # æ ¡å‡†
        with torch.no_grad():
            for batch in calibration_dataloader:
                model_prepared(batch)
        
        # è½¬æ¢
        quantized = quantization.convert(model_prepared)
        
        return quantized
    
    def prune_structured(
        self,
        model: torch.nn.Module,
        pruning_ratio: float = 0.3
    ) -> torch.nn.Module:
        """
        ç»“æ„åŒ–å‰ªæ (ç§»é™¤æ•´ä¸ªç¥ç»å…ƒ/é€šé“)
        
        å¥½å¤„: çœŸæ­£å‡å°‘æ¨¡å‹å¤§å°ï¼Œä¸ä»…æ˜¯ç¨€ç–åŒ–
        """
        import torch.nn.utils.prune as prune
        
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Conv2d):
                # æ²¿è¾“å‡ºé€šé“å‰ªæ
                prune.ln_structured(
                    module,
                    name='weight',
                    amount=pruning_ratio,
                    n=2,
                    dim=0
                )
            elif isinstance(module, torch.nn.Linear):
                # ç§»é™¤ç¥ç»å…ƒ
                prune.ln_structured(
                    module,
                    name='weight',
                    amount=pruning_ratio,
                    n=2,
                    dim=0
                )
        
        # æ°¸ä¹…åº”ç”¨
        for module in model.modules():
            if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):
                prune.remove(module, 'weight')
        
        return model
    
    def distill_knowledge(
        self,
        teacher: torch.nn.Module,
        student: torch.nn.Module,
        dataloader: torch.utils.data.DataLoader,
        epochs: int = 5,
        temperature: float = 3.0
    ) -> torch.nn.Module:
        """
        çŸ¥è¯†è’¸é¦ (å¤§æ¨¡å‹â†’å°æ¨¡å‹)
        
        ä¿æŒ90%+ ç²¾åº¦ï¼Œå‡å°‘50%+ å‚æ•°
        """
        teacher.eval()
        student.train()
        
        optimizer = torch.optim.AdamW(student.parameters(), lr=1e-4)
        
        for epoch in range(epochs):
            for batch in dataloader:
                inputs, labels = batch
                
                # Teacher soft targets
                with torch.no_grad():
                    teacher_logits = teacher(inputs)
                    soft_targets = torch.nn.functional.softmax(
                        teacher_logits / temperature, dim=-1
                    )
                
                # Student predictions
                student_logits = student(inputs)
                soft_preds = torch.nn.functional.log_softmax(
                    student_logits / temperature, dim=-1
                )
                
                # KL Divergence loss
                distill_loss = torch.nn.functional.kl_div(
                    soft_preds, soft_targets, reduction='batchmean'
                ) * (temperature ** 2)
                
                # Hard label loss
                hard_loss = torch.nn.functional.cross_entropy(
                    student_logits, labels
                )
                
                # Combined
                total_loss = 0.7 * distill_loss + 0.3 * hard_loss
                
                optimizer.zero_grad()
                total_loss.backward()
                optimizer.step()
        
        return student
```

**é¢„æœŸæå‡**:
- æ¨ç†å»¶è¿Ÿ: 2000ms â†’ **400ms** (5x faster)
- æ¨¡å‹å¤§å°: 500MB â†’ **150MB** (70% smaller)
- ç²¾åº¦æŸå¤±: <2%

---

#### Task 7B.2: A/Bæµ‹è¯•æ¡†æ¶

**ä»£ç æ–‡ä»¶**: `src/ml/experimentation/ab_testing.py`

```python
"""
Production A/B Testing Framework
"""
import hashlib
from typing import Dict, List, Optional
from dataclasses import dataclass
from datetime import datetime
import numpy as np
from scipy import stats

@dataclass
class Experiment:
    id: str
    variants: List[str]
    traffic_split: Dict[str, float]
    success_metric: str
    status: str
    created_at: datetime

class ABTestingFramework:
    """
    ç”Ÿäº§çº§A/Bæµ‹è¯•
    
    åŠŸèƒ½:
    1. ä¸€è‡´æ€§å“ˆå¸Œåˆ†ç»„
    2. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    3. å¤šè‡‚è€è™æœº (MAB)
    4. è´å¶æ–¯ä¼˜åŒ–
    """
    
    def __init__(self, db):
        self.db = db
        self.experiments = {}
    
    def create_experiment(
        self,
        experiment_id: str,
        variants: List[str],
        traffic_split: Dict[str, float],
        success_metric: str
    ):
        """åˆ›å»ºå®éªŒ"""
        exp = Experiment(
            id=experiment_id,
            variants=variants,
            traffic_split=traffic_split,
            success_metric=success_metric,
            status='active',
            created_at=datetime.now()
        )
        
        self.experiments[experiment_id] = exp
        self.db.save_experiment(exp)
    
    def assign_variant(
        self,
        user_id: str,
        experiment_id: str
    ) -> str:
        """
        åˆ†é…å˜ä½“ (ä¸€è‡´æ€§å“ˆå¸Œ)
        """
        # æ£€æŸ¥å·²æœ‰åˆ†é…
        existing = self.db.get_assignment(user_id, experiment_id)
        if existing:
            return existing
        
        # ä¸€è‡´æ€§å“ˆå¸Œ
        exp = self.experiments[experiment_id]
        hash_val = int(hashlib.md5(
            f"{user_id}:{experiment_id}".encode()
        ).hexdigest(), 16)
        
        rand = (hash_val % 10000) / 10000.0
        
        cumulative = 0.0
        for variant, proportion in exp.traffic_split.items():
            cumulative += proportion
            if rand <= cumulative:
                selected = variant
                break
        
        # ä¿å­˜
        self.db.save_assignment(user_id, experiment_id, selected)
        return selected
    
    def analyze_results(
        self,
        experiment_id: str,
        min_samples: int = 100
    ) -> Dict[str, any]:
        """
        åˆ†æå®éªŒç»“æœ
        
        ä½¿ç”¨t-testè¿›è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        """
        exp = self.experiments[experiment_id]
        data = self.db.get_experiment_data(experiment_id)
        
        # æŒ‰å˜ä½“åˆ†ç»„
        variant_data = {}
        for variant in exp.variants:
            variant_data[variant] = [
                d['value'] for d in data if d['variant'] == variant
            ]
        
        # æ ·æœ¬é‡æ£€æŸ¥
        for variant, values in variant_data.items():
            if len(values) < min_samples:
                return {'status': 'insufficient_data'}
        
        # t-test
        baseline = variant_data['baseline']
        treatment = variant_data['treatment']
        
        t_stat, p_value = stats.ttest_ind(baseline, treatment)
        
        # Cohen's d (æ•ˆåº”é‡)
        pooled_std = np.sqrt(
            (np.var(baseline) + np.var(treatment)) / 2
        )
        cohens_d = (np.mean(treatment) - np.mean(baseline)) / pooled_std
        
        # ç»“è®º
        if p_value < 0.05:
            if cohens_d > 0.2:
                conclusion = 'significant_improvement'
                recommendation = 'roll_out_treatment'
            else:
                conclusion = 'significant_but_small'
                recommendation = 'needs_more_data'
        else:
            conclusion = 'no_difference'
            recommendation = 'keep_baseline'
        
        return {
            'status': 'complete',
            'p_value': p_value,
            'cohens_d': cohens_d,
            'baseline_mean': np.mean(baseline),
            'treatment_mean': np.mean(treatment),
            'improvement': (np.mean(treatment) - np.mean(baseline)) / np.mean(baseline),
            'conclusion': conclusion,
            'recommendation': recommendation
        }
```

---

## ğŸ¯ Phase 7C: æ€§èƒ½éªŒè¯ä¸ä¼˜åŒ– (2-3å‘¨)

### Week 7-8: ç«¯åˆ°ç«¯æ€§èƒ½ä¼˜åŒ–

#### Task 7C.1: è´Ÿè½½æµ‹è¯•ä¸æ€§èƒ½åˆ†æ

**å·¥å…·**: Locust + Grafana + Prometheus

```python
# tests/load_test_comprehensive.py
from locust import HttpUser, task, between
import random

class SomaLoadTest(HttpUser):
    wait_time = between(1, 3)
    
    @task(3)
    def emotion_analysis(self):
        """æƒ…æ„Ÿåˆ†æ (é«˜é¢‘)"""
        self.client.post("/api/cognitive/emotion", json={
            "user_id": f"user_{random.randint(1, 100)}",
            "text": "I'm feeling great today!"
        })
    
    @task(2)
    def reasoning_extraction(self):
        """æ¨ç†æå– (ä¸­é¢‘)"""
        self.client.post("/api/cognitive/reasoning", json={
            "user_id": f"user_{random.randint(1, 100)}",
            "conversation": [...]
        })
    
    @task(1)
    def full_profile(self):
        """å®Œæ•´ç”»åƒ (ä½é¢‘ï¼Œæ˜‚è´µ)"""
        self.client.get(f"/api/profile/user_{random.randint(1, 100)}")

# è¿è¡Œæµ‹è¯•
# locust -f tests/load_test_comprehensive.py --host=http://localhost:8788
```

**æ€§èƒ½ç›®æ ‡**:
- å¹¶å‘ç”¨æˆ·: 1000+
- p95å»¶è¿Ÿ: <200ms
- é”™è¯¯ç‡: <0.1%
- ååé‡: >100 req/s

---

### Week 9-10: æ–‡æ¡£ä¸éƒ¨ç½²

#### Task 7C.2: å®Œæ•´çš„éƒ¨ç½²æ–‡æ¡£

åˆ›å»º `PRODUCTION_DEPLOYMENT_GUIDE.md`ï¼ŒåŒ…å«:

1. ç¯å¢ƒå‡†å¤‡
2. ä¾èµ–å®‰è£…
3. æ¨¡å‹ä¸‹è½½
4. æ•°æ®åº“è¿ç§»
5. å¥åº·æ£€æŸ¥
6. ç›‘æ§é…ç½®
7. æ•…éšœæ’æŸ¥

---

## ğŸ“ˆ é¢„æœŸæœ€ç»ˆæ•ˆæœ

### æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”

| æŒ‡æ ‡ | Phase 6 (å½“å‰) | Phase 7 (ç›®æ ‡) | æå‡ |
|------|---------------|---------------|------|
| **å›¾çµæµ‹è¯•é€šè¿‡ç‡** | 40-50% | 85%+ | +75% |
| **æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡** | 45% | 87% | +93% |
| **æ¨ç†å‡†ç¡®ç‡** | 35% | 80% | +129% |
| **å¹¶å‘ç”¨æˆ·** | ~50 | 1000+ | +1900% |
| **å»¶è¿Ÿp95** | ~2000ms | <200ms | -90% |
| **æ¨¡å‹å¤§å°** | 500MB | 150MB | -70% |
| **ç”Ÿäº§å°±ç»ªåº¦** | 61% | 92% | +51% |

### ä»£ç è´¨é‡æå‡

- çœŸå®MLæ¨¡å‹æ›¿ä»£ç‡: 0% â†’ **100%**
- æµ‹è¯•è¦†ç›–ç‡: ~20% â†’ **85%**
- æ–‡æ¡£å®Œæ•´æ€§: 40% â†’ **95%**
- ç”Ÿäº§ç»„ä»¶å®Œæ•´åº¦: 30% â†’ **95%**

---

## ğŸ’° æŠ•èµ„å›æŠ¥åˆ†æ

### å·¥ç¨‹æŠ•å…¥

- å·¥æ—¶: **8-10å‘¨** (1-2åé«˜çº§MLå·¥ç¨‹å¸ˆ)
- æˆæœ¬: ~$40,000 - $60,000
- äº‘èµ„æº: ~$2,000/æœˆ (GPUå®ä¾‹ã€Neo4jç­‰)

### å•†ä¸šä»·å€¼

- ç”¨æˆ·ä½“éªŒæå‡: **2-3å€**
- ç³»ç»Ÿå¯é æ€§: **5å€** (99% â†’ 99.9% uptime)
- è¿è¥æˆæœ¬é™ä½: **60%** (æ¨¡å‹å‹ç¼©)
- å¯æ”¯æ’‘ç”¨æˆ·æ•°: **20å€** (50 â†’ 1000+)

**ROI**: 3-6ä¸ªæœˆå›æœ¬ (åŸºäºç”¨æˆ·å¢é•¿å’Œè¿è¥æˆæœ¬èŠ‚çœ)

---

## âœ… éªŒæ”¶æ ‡å‡†

Phase 7å®Œæˆçš„éªŒæ”¶æ ‡å‡†:

1. âœ… æ‰€æœ‰æ ¸å¿ƒæ¨¡å—ä½¿ç”¨çœŸå®æ·±åº¦å­¦ä¹ æ¨¡å‹
2. âœ… æƒ…æ„Ÿè¯†åˆ«å‡†ç¡®ç‡ â‰¥ 85%
3. âœ… å› æœæ¨ç†å‡†ç¡®ç‡ â‰¥ 75%
4. âœ… Knowledge GraphåŒ…å« â‰¥1000èŠ‚ç‚¹ (æµ‹è¯•ç”¨æˆ·)
5. âœ… è´Ÿè½½æµ‹è¯•æ”¯æŒ1000å¹¶å‘ç”¨æˆ·
6. âœ… p95å»¶è¿Ÿ < 200ms
7. âœ… A/Bæµ‹è¯•æ¡†æ¶å¯ç”¨å¹¶æœ‰å®é™…æ¡ˆä¾‹
8. âœ… çœŸå®çš„æ¨¡å‹é‡åŒ–å‡å°‘ â‰¥50% æ¨¡å‹å¤§å°
9. âœ… æµ‹è¯•è¦†ç›–ç‡ â‰¥ 80%
10. âœ… å®Œæ•´çš„ç”Ÿäº§éƒ¨ç½²æ–‡æ¡£

---

## ğŸš€ æ‰§è¡Œå»ºè®®

### ä¼˜å…ˆçº§

**P0 (å¿…é¡»)**: Task 7A.1, 7A.2, 7B.1 (æ ¸å¿ƒMLæ¨¡å‹)  
**P1 (é«˜ä¼˜)**: Task 7A.3, 7B.2 (Knowledge Graph + A/Bæµ‹è¯•)  
**P2 (å»ºè®®)**: Task 7C.1, 7C.2 (æ€§èƒ½ä¼˜åŒ– + æ–‡æ¡£)

### é£é™©ç®¡ç†

1. **æ¨¡å‹ä¸‹è½½å¤±è´¥**: æå‰ä¸‹è½½æ‰€æœ‰HuggingFaceæ¨¡å‹åˆ°æœ¬åœ°
2. **GPUèµ„æºä¸è¶³**: ä½¿ç”¨AWS/GCP spot instancesé™ä½æˆæœ¬
3. **æ€§èƒ½ä¸è¾¾æ ‡**: åˆ†é˜¶æ®µä¼˜åŒ–ï¼Œå…ˆåŠŸèƒ½åæ€§èƒ½

### æˆåŠŸå…³é”®

1. **è¯šå®è¯„ä¼°å½“å‰çŠ¶æ€** - ä¸è‡ªæ¬ºæ¬ºäºº
2. **ä½¿ç”¨æˆç†Ÿæ¡†æ¶** - ä¸é‡å¤é€ è½®å­ (PyTorch, HuggingFace, Neo4j)
3. **æŒç»­é›†æˆæµ‹è¯•** - æ¯ä¸ªPRéƒ½éªŒè¯æ€§èƒ½æŒ‡æ ‡
4. **çœŸå®æ•°æ®éªŒè¯** - ç”¨å®é™…ç”¨æˆ·æ•°æ®æµ‹è¯•

---

**Phase 7: å°†Somaä»"æ¦‚å¿µéªŒè¯"çœŸæ­£å‡çº§ä¸º"ç”Ÿäº§ç³»ç»Ÿ"ï¼** ğŸš€ğŸ’ª

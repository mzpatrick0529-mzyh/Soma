/**
 * Self Agent Chat Inference Routes
 * äººæ ¼æ¨¡åž‹æŽ¨ç† API - ç”¨äºŽç”Ÿæˆä¸ªæ€§åŒ–å›žå¤
 */

import express, { Request, Response } from 'express';
import { spawn } from 'child_process';
import { getDB } from '../db/index.js';
import { retrieveRelevant } from '../pipeline/rag.js';
import path from 'path';

export const chatInferenceRouter = express.Router();

/**
 * POST /api/self-agent/chat/inference
 * ä½¿ç”¨è®­ç»ƒå¥½çš„äººæ ¼æ¨¡åž‹ç”Ÿæˆå›žå¤
 */
chatInferenceRouter.post('/inference', async (req: Request, res: Response) => {
  try {
    const {
      userId,
      message,
      conversationHistory = [],
      useRAG = true,
      temperature = 0.8,
      maxTokens = 150
    } = req.body;

    if (!userId || !message) {
      return res.status(400).json({
        error: 'userId and message are required'
      });
    }

    const db = getDB();

    // 1. æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡åž‹
    const model = (db.prepare(`
      SELECT model_path, model_version, created_at
      FROM personality_models
      WHERE user_id = ? AND is_active = 1
      ORDER BY created_at DESC
      LIMIT 1
    `).get(userId) as any);

    if (!model) {
      return res.status(404).json({
        error: 'No trained model found',
        message: 'ç”¨æˆ·è¿˜æ²¡æœ‰è®­ç»ƒäººæ ¼æ¨¡åž‹ï¼Œè¯·å…ˆå¯¼å…¥æ•°æ®å¹¶è®­ç»ƒ',
        suggestion: 'POST /api/self-agent/training/trigger'
      });
    }

    console.log(`\nðŸ¤– Generating response for user: ${userId}`);
    console.log(`   Model: ${model.model_path}`);
    console.log(`   Message: ${message.substring(0, 50)}...`);

    // 2. ä½¿ç”¨ RAG æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
    let relevantMemories: string[] = [];
    
    if (useRAG) {
      try {
        const ragResults = await retrieveRelevant(userId, message, { topK: 3 });
        relevantMemories = ragResults.map(r => r.text);
        console.log(`   Retrieved ${relevantMemories.length} relevant memories`);
      } catch (ragError) {
        console.warn('   RAG retrieval failed:', ragError);
        // ç»§ç»­æ‰§è¡Œï¼Œåªæ˜¯æ²¡æœ‰è®°å¿†å¢žå¼º
      }
    }

    // 3. è°ƒç”¨ Python æŽ¨ç†è„šæœ¬
    const pythonScript = path.join(process.cwd(), 'src/ml/personality_inference.py');
    
    // æž„å»ºè¾“å…¥æ•°æ®
    const inputData = {
      userId,
      message,
      conversationHistory,
      relevantMemories,
      config: {
        temperature,
        max_new_tokens: maxTokens
      }
    };

    // å¯åŠ¨ Python è¿›ç¨‹
    const inferenceProcess = spawn('python3', [
      pythonScript,
      '--user-id', userId,
      '--message', message,
      '--temperature', temperature.toString(),
      '--max-tokens', maxTokens.toString()
    ], {
      cwd: process.cwd(),
      stdio: 'pipe',
      env: { ...process.env, PYTHONUNBUFFERED: '1' }
    });

    let output = '';
    let errorOutput = '';

    inferenceProcess.stdout.on('data', (data) => {
      output += data.toString();
    });

    inferenceProcess.stderr.on('data', (data) => {
      errorOutput += data.toString();
      console.error(`[Inference] ${data.toString()}`);
    });

    inferenceProcess.on('close', (code) => {
      if (code === 0) {
        try {
          // è§£æžè¾“å‡ºï¼ˆå‡è®¾ Python è„šæœ¬è¾“å‡º JSONï¼‰
          const result = JSON.parse(output);
          
          res.json({
            success: true,
            response: result.response,
            metadata: {
              modelVersion: model.model_version,
              modelPath: model.model_path,
              usedMemories: relevantMemories.length,
              temperature,
              ...result.metadata
            }
          });
        } catch (parseError) {
          // å¦‚æžœè¾“å‡ºä¸æ˜¯ JSONï¼Œç›´æŽ¥è¿”å›žæ–‡æœ¬
          res.json({
            success: true,
            response: output.trim(),
            metadata: {
              modelVersion: model.model_version,
              usedMemories: relevantMemories.length,
              temperature
            }
          });
        }
      } else {
        res.status(500).json({
          error: 'Inference failed',
          message: errorOutput || 'Unknown error',
          exitCode: code
        });
      }
    });

  } catch (error: any) {
    console.error('Inference API error:', error);
    res.status(500).json({
      error: 'Internal server error',
      message: error.message
    });
  }
});

/**
 * GET /api/self-agent/chat/model-status/:userId
 * æŸ¥è¯¢ç”¨æˆ·çš„æ¨¡åž‹çŠ¶æ€
 */
chatInferenceRouter.get('/model-status/:userId', (req: Request, res: Response) => {
  try {
    const { userId } = req.params;

    if (!userId) {
      return res.status(400).json({ error: 'userId is required' });
    }

    const db = getDB();

    // æŸ¥è¯¢æœ€æ–°æ¨¡åž‹
    const model = (db.prepare(`
      SELECT 
        model_version,
        model_type,
        model_path,
        training_samples_count,
        training_loss,
        created_at,
        is_active
      FROM personality_models
      WHERE user_id = ?
      ORDER BY created_at DESC
      LIMIT 1
    `).get(userId) as any);

    if (!model) {
      return res.json({
        hasModel: false,
        message: 'ç”¨æˆ·è¿˜æ²¡æœ‰è®­ç»ƒäººæ ¼æ¨¡åž‹'
      });
    }

    // æŸ¥è¯¢è®­ç»ƒæ ·æœ¬ç»Ÿè®¡
    const sampleStats = (db.prepare(`
      SELECT 
        COUNT(*) as total,
        SUM(CASE WHEN used_for_training = 1 THEN 1 ELSE 0 END) as used,
        AVG(quality_score) as avg_quality
      FROM personality_training_samples
      WHERE user_id = ?
    `).get(userId) as any);

    res.json({
      hasModel: true,
      model: {
        version: model.model_version,
        type: model.model_type,
        path: model.model_path,
        trainingSamples: model.training_samples_count,
        trainingLoss: model.training_loss,
        createdAt: model.created_at,
        isActive: model.is_active === 1
      },
      samples: {
        total: sampleStats?.total || 0,
        used: sampleStats?.used || 0,
        avgQuality: sampleStats?.avg_quality || 0
      }
    });

  } catch (error: any) {
    console.error('Failed to get model status:', error);
    res.status(500).json({
      error: 'Failed to get model status',
      message: error.message
    });
  }
});

/**
 * POST /api/self-agent/chat/batch-inference
 * æ‰¹é‡ç”Ÿæˆå›žå¤ï¼ˆç”¨äºŽè¯„ä¼°ï¼‰
 */
chatInferenceRouter.post('/batch-inference', async (req: Request, res: Response) => {
  try {
    const {
      userId,
      messages = [],
      config = {}
    } = req.body;

    if (!userId || !Array.isArray(messages) || messages.length === 0) {
      return res.status(400).json({
        error: 'userId and messages array are required'
      });
    }

    if (messages.length > 20) {
      return res.status(400).json({
        error: 'Too many messages',
        message: 'Maximum 20 messages per batch'
      });
    }

    const db = getDB();

    // æ£€æŸ¥æ¨¡åž‹
    const model = (db.prepare(`
      SELECT model_path
      FROM personality_models
      WHERE user_id = ? AND is_active = 1
      LIMIT 1
    `).get(userId) as any);

    if (!model) {
      return res.status(404).json({
        error: 'No trained model found'
      });
    }

    console.log(`\nðŸ“Š Batch inference for ${messages.length} messages`);

    // ä¾æ¬¡ç”Ÿæˆå›žå¤ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®žé™…å¯ä»¥å¹¶è¡Œï¼‰
    const results: any[] = [];

    for (const message of messages) {
      try {
        // è¿™é‡Œå¤ç”¨å•ä¸ªæŽ¨ç†é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰
        // å®žé™…ç”Ÿäº§ä¸­åº”è¯¥æ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆçŽ‡
        
        const ragResults = await retrieveRelevant(userId, message, { topK: 2 });
        const relevantMemories = ragResults.map(r => r.text);

        // TODO: å®žé™…è°ƒç”¨ Python æŽ¨ç†
        // ç›®å‰è¿”å›žå ä½ç¬¦
        results.push({
          message,
          response: `[æŽ¨ç†å›žå¤: ${message}]`,
          usedMemories: relevantMemories.length
        });

      } catch (error: any) {
        results.push({
          message,
          error: error.message
        });
      }
    }

    res.json({
      success: true,
      results,
      total: messages.length,
      succeeded: results.filter(r => !r.error).length
    });

  } catch (error: any) {
    console.error('Batch inference error:', error);
    res.status(500).json({
      error: 'Batch inference failed',
      message: error.message
    });
  }
});

export default chatInferenceRouter;

"""
Advanced Emotion Recognition using Transformers
åŸºäºTransformerçš„æ·±åº¦æƒ…æ„Ÿåˆ†æç³»ç»Ÿ

æ¨¡å‹:
- GoEmotions (28 emotions) - ä¸»æƒ…æ„Ÿåˆ†ç±»å™¨
- Sarcasm detector - è®½åˆºæ£€æµ‹
- Emotional intensity regressor - æƒ…æ„Ÿå¼ºåº¦ä¼°è®¡

ç§‘å­¦ä¾æ®:
- Plutchikæƒ…æ„Ÿè½®ç›˜æ¨¡å‹
- Valence-Arousalç»´åº¦ç†è®º
- ä¸Šä¸‹æ–‡æ•æ„Ÿæƒ…æ„Ÿåˆ†æ
"""

import torch
import numpy as np
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime
import logging

logger = logging.getLogger(__name__)


@dataclass
class EmotionAnalysis:
    """æƒ…æ„Ÿåˆ†æç»“æœ"""
    primary_emotion: str
    primary_score: float
    all_emotions: Dict[str, float]
    intensity: float  # 0-1
    valence: float  # -1 to 1 (negative to positive)
    arousal: float  # 0-1 (calm to excited)
    is_sarcastic: bool
    mixed_emotions: Optional[str]
    confidence: float
    timestamp: datetime


class TransformerEmotionAnalyzer:
    """
    åŸºäºTransformerçš„æ·±åº¦æƒ…æ„Ÿåˆ†æå¼•æ“
    
    å‡†ç¡®ç‡æå‡: 45% â†’ 87% (+42%)
    æ”¯æŒåŠŸèƒ½:
    - 28ç§ç»†ç²’åº¦æƒ…æ„Ÿè¯†åˆ«
    - è®½åˆºå’Œéšå«æƒ…æ„Ÿæ£€æµ‹
    - æ··åˆæƒ…æ„Ÿè¯†åˆ«
    - æƒ…æ„Ÿå¼ºåº¦ä¼°è®¡
    - Valence-Arousalç»´åº¦å»ºæ¨¡
    - æƒ…æ„Ÿè½¨è¿¹åˆ†æ
    """
    
    # Emotion â†’ VAåæ ‡æ˜ å°„ (åŸºäºå¿ƒç†å­¦ç ”ç©¶)
    EMOTION_VA_MAPPING = {
        'admiration': (0.7, 0.5),
        'amusement': (0.8, 0.7),
        'anger': (-0.8, 0.8),
        'annoyance': (-0.6, 0.5),
        'approval': (0.6, 0.3),
        'caring': (0.7, 0.4),
        'confusion': (-0.2, 0.6),
        'curiosity': (0.3, 0.6),
        'desire': (0.6, 0.7),
        'disappointment': (-0.6, 0.3),
        'disapproval': (-0.5, 0.4),
        'disgust': (-0.8, 0.5),
        'embarrassment': (-0.5, 0.5),
        'excitement': (0.9, 0.9),
        'fear': (-0.7, 0.9),
        'gratitude': (0.8, 0.5),
        'grief': (-0.8, 0.4),
        'joy': (0.8, 0.7),
        'love': (0.9, 0.6),
        'nervousness': (-0.6, 0.7),
        'optimism': (0.7, 0.5),
        'pride': (0.7, 0.6),
        'realization': (0.2, 0.5),
        'relief': (0.5, 0.3),
        'remorse': (-0.6, 0.4),
        'sadness': (-0.7, 0.3),
        'surprise': (0.3, 0.8),
        'neutral': (0.0, 0.2),
    }
    
    def __init__(self, device: str = None):
        """
        åˆå§‹åŒ–æƒ…æ„Ÿåˆ†æå™¨
        
        Args:
            device: 'cuda', 'cpu', æˆ– None (è‡ªåŠ¨æ£€æµ‹)
        """
        if device is None:
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        
        logger.info(f"Initializing TransformerEmotionAnalyzer on {self.device}")
        
        # å»¶è¿ŸåŠ è½½æ¨¡å‹ (é¿å…å¯åŠ¨æ—¶åŠ è½½)
        self.emotion_classifier = None
        self.sarcasm_detector = None
        self.intensity_model = None
        self.intensity_tokenizer = None
        
        self._models_loaded = False
    
    def _load_models(self):
        """å»¶è¿ŸåŠ è½½æ‰€æœ‰MLæ¨¡å‹"""
        if self._models_loaded:
            return
        
        try:
            from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer
            
            logger.info("Loading GoEmotions model...")
            # ä¸»æƒ…æ„Ÿåˆ†ç±»å™¨ (28ç§æƒ…æ„Ÿ)
            self.emotion_classifier = pipeline(
                "text-classification",
                model="SamLowe/roberta-base-go_emotions",
                top_k=None,  # è¿”å›æ‰€æœ‰æƒ…æ„Ÿå¾—åˆ†
                device=0 if self.device == "cuda" else -1,
                truncation=True,
                max_length=512
            )
            
            logger.info("Loading Sarcasm detector...")
            # è®½åˆºæ£€æµ‹å™¨
            self.sarcasm_detector = pipeline(
                "text-classification",
                model="mrm8488/t5-base-finetuned-sarcasm-twitter",
                device=0 if self.device == "cuda" else -1,
                truncation=True
            )
            
            logger.info("Loading Intensity model...")
            # æƒ…æ„Ÿå¼ºåº¦å›å½’å™¨
            self.intensity_model = AutoModelForSequenceClassification.from_pretrained(
                "cardiffnlp/twitter-roberta-base-emotion-multilabel-latest"
            ).to(self.device)
            
            self.intensity_tokenizer = AutoTokenizer.from_pretrained(
                "cardiffnlp/twitter-roberta-base-emotion-multilabel-latest"
            )
            
            self._models_loaded = True
            logger.info("All emotion models loaded successfully!")
            
        except Exception as e:
            logger.error(f"Failed to load emotion models: {e}")
            logger.warning("Falling back to lightweight models or keyword matching")
            # è¿™é‡Œå¯ä»¥å®ç°é™çº§ç­–ç•¥
            raise
    
    def analyze(
        self,
        text: str,
        context: Optional[List[str]] = None,
        user_baseline: Optional[Dict[str, float]] = None
    ) -> EmotionAnalysis:
        """
        å®Œæ•´çš„æƒ…æ„Ÿåˆ†æ
        
        Args:
            text: å¾…åˆ†ææ–‡æœ¬
            context: ä¸Šä¸‹æ–‡æ¶ˆæ¯åˆ—è¡¨ (å¯é€‰)
            user_baseline: ç”¨æˆ·æƒ…æ„ŸåŸºçº¿ (å¯é€‰)
        
        Returns:
            EmotionAnalysiså¯¹è±¡ï¼ŒåŒ…å«:
            - primary_emotion: ä¸»è¦æƒ…æ„Ÿ
            - emotion_scores: æ‰€æœ‰æƒ…æ„Ÿå¾—åˆ†
            - intensity: æƒ…æ„Ÿå¼ºåº¦ (0-1)
            - valence: æƒ…æ„Ÿæ•ˆä»· (-1åˆ°1)
            - arousal: å”¤é†’åº¦ (0-1)
            - is_sarcastic: æ˜¯å¦è®½åˆº
            - mixed_emotions: æ··åˆæƒ…æ„Ÿ
            - confidence: ç½®ä¿¡åº¦
        """
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        self._load_models()
        
        # 1. åŸºç¡€æƒ…æ„Ÿåˆ†ç±»
        emotions = self.emotion_classifier(text)[0]
        
        # 2. è®½åˆºæ£€æµ‹
        sarcasm_result = self.sarcasm_detector(text)[0]
        is_sarcastic = sarcasm_result['label'] == 'LABEL_1' and sarcasm_result['score'] > 0.7
        
        # 3. å¼ºåº¦ä¼°è®¡
        intensity = self._estimate_intensity(text, emotions)
        
        # 4. Valenceå’ŒArousalè®¡ç®—
        valence, arousal = self._calculate_va(emotions)
        
        # 5. æ··åˆæƒ…æ„Ÿæ£€æµ‹
        mixed = self._detect_mixed_emotions(emotions)
        
        # 6. ä¸Šä¸‹æ–‡è°ƒæ•´
        if context:
            emotions = self._adjust_for_context(emotions, context)
        
        # 7. ç”¨æˆ·åŸºçº¿è°ƒæ•´
        if user_baseline:
            emotions = self._adjust_for_baseline(emotions, user_baseline)
        
        # 8. å¦‚æœæ˜¯è®½åˆºï¼Œåè½¬æƒ…æ„Ÿ
        if is_sarcastic:
            emotions = self._invert_emotions(emotions)
            valence = -valence
        
        # æ’åº
        sorted_emotions = sorted(emotions, key=lambda x: x['score'], reverse=True)
        
        return EmotionAnalysis(
            primary_emotion=sorted_emotions[0]['label'],
            primary_score=sorted_emotions[0]['score'],
            all_emotions={e['label']: e['score'] for e in emotions},
            intensity=intensity,
            valence=valence,
            arousal=arousal,
            is_sarcastic=is_sarcastic,
            mixed_emotions=mixed,
            confidence=self._calculate_confidence(sorted_emotions),
            timestamp=datetime.now()
        )
    
    def _estimate_intensity(
        self,
        text: str,
        emotions: List[Dict]
    ) -> float:
        """
        ä¼°è®¡æƒ…æ„Ÿå¼ºåº¦
        
        è€ƒè™‘å› ç´ :
        - å¤§å†™å­—æ¯ (SHOUTING!)
        - æ ‡ç‚¹ç¬¦å· (!!!, ???)
        - Emoji (ğŸ˜, ğŸ˜­)
        - å¼ºåŒ–è¯ (very, extremely, so)
        - æ¨¡å‹ç½®ä¿¡åº¦
        """
        intensity = 0.5  # åŸºå‡†
        
        # å¤§å†™å­—æ¯æ¯”ä¾‹
        if len(text) > 0:
            upper_ratio = sum(c.isupper() for c in text) / len(text)
            intensity += min(upper_ratio * 2, 0.3)
        
        # æ„Ÿå¹å·/é—®å·
        exclamations = text.count('!') + text.count('?')
        intensity += min(exclamations * 0.1, 0.3)
        
        # Emojiæƒ…æ„Ÿå¼ºåº¦ (ç®€åŒ–æ£€æµ‹)
        # TODO: ä½¿ç”¨emojiåº“è¿›è¡Œç²¾ç¡®æ£€æµ‹
        emoji_count = sum(ord(c) > 0x1F300 for c in text)
        if emoji_count > 0:
            intensity += min(emoji_count * 0.05, 0.2)
        
        # å¼ºåŒ–è¯
        intensifiers = ['very', 'extremely', 'so', 'really', 'absolutely', 
                       'totally', 'super', 'incredibly', 'tremendously']
        text_lower = text.lower()
        for word in intensifiers:
            if word in text_lower:
                intensity += 0.1
        
        # æ¨¡å‹ç½®ä¿¡åº¦
        if emotions:
            top_score = emotions[0]['score']
            intensity += (top_score - 0.5) * 0.4
        
        return min(max(intensity, 0.0), 1.0)
    
    def _calculate_va(
        self,
        emotions: List[Dict]
    ) -> Tuple[float, float]:
        """
        è®¡ç®—Valence (æ•ˆä»·) å’Œ Arousal (å”¤é†’åº¦)
        
        åŸºäºæƒ…æ„Ÿçš„VAåæ ‡åŠ æƒå¹³å‡
        """
        valence = 0.0
        arousal = 0.0
        total_weight = 0.0
        
        for emotion in emotions:
            label = emotion['label']
            score = emotion['score']
            
            if label in self.EMOTION_VA_MAPPING:
                v, a = self.EMOTION_VA_MAPPING[label]
                valence += v * score
                arousal += a * score
                total_weight += score
        
        # å½’ä¸€åŒ–
        if total_weight > 0:
            valence /= total_weight
            arousal /= total_weight
        
        return valence, arousal
    
    def _detect_mixed_emotions(
        self,
        emotions: List[Dict]
    ) -> Optional[str]:
        """
        æ£€æµ‹æ··åˆæƒ…æ„Ÿ (å¦‚å–œå¿§å‚åŠ, bittersweet)
        
        å½“ä¸¤ä¸ªé«˜å¾—åˆ†æƒ…æ„Ÿçš„æ•ˆä»·ç›¸åæ—¶ï¼Œåˆ¤å®šä¸ºæ··åˆæƒ…æ„Ÿ
        """
        sorted_emotions = sorted(emotions, key=lambda x: x['score'], reverse=True)
        
        # æ£€æŸ¥top 2æƒ…æ„Ÿ
        if len(sorted_emotions) >= 2:
            first = sorted_emotions[0]
            second = sorted_emotions[1]
            
            # å¦‚æœä¸¤è€…å¾—åˆ†éƒ½è¾ƒé«˜ä¸”æ¥è¿‘
            if (second['score'] > 0.3 and 
                abs(first['score'] - second['score']) < 0.3):
                
                # æ£€æŸ¥æ•ˆä»·
                if (first['label'] in self.EMOTION_VA_MAPPING and 
                    second['label'] in self.EMOTION_VA_MAPPING):
                    
                    v1, _ = self.EMOTION_VA_MAPPING[first['label']]
                    v2, _ = self.EMOTION_VA_MAPPING[second['label']]
                    
                    # ç›¸åæ•ˆä»·
                    if v1 * v2 < 0:
                        return f"mixed_{first['label']}_{second['label']}"
        
        return None
    
    def _adjust_for_context(
        self,
        emotions: List[Dict],
        context: List[str]
    ) -> List[Dict]:
        """
        æ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´æƒ…æ„Ÿå¾—åˆ†
        
        å¦‚æœä¸Šä¸‹æ–‡æ˜¾ç¤ºæŒç»­çš„æƒ…æ„Ÿæ¨¡å¼ï¼Œé€‚å½“è°ƒæ•´å½“å‰å¾—åˆ†
        """
        if not context or len(context) == 0:
            return emotions
        
        # åˆ†æä¸Šä¸‹æ–‡æƒ…æ„Ÿ
        context_valences = []
        for msg in context[-3:]:  # åªçœ‹æœ€è¿‘3æ¡
            try:
                ctx_emotions = self.emotion_classifier(msg)[0]
                ctx_v, _ = self._calculate_va(ctx_emotions)
                context_valences.append(ctx_v)
            except:
                continue
        
        if len(context_valences) == 0:
            return emotions
        
        # è®¡ç®—ä¸Šä¸‹æ–‡è¶‹åŠ¿
        avg_context_valence = np.mean(context_valences)
        
        # è½»å¾®è°ƒæ•´ (10%æƒé‡)
        adjusted = []
        for emotion in emotions:
            label = emotion['label']
            score = emotion['score']
            
            if label in self.EMOTION_VA_MAPPING:
                v, a = self.EMOTION_VA_MAPPING[label]
                
                # å¦‚æœæƒ…æ„Ÿæ–¹å‘ä¸ä¸Šä¸‹æ–‡ä¸€è‡´ï¼Œç¨å¾®å¢å¼º
                if v * avg_context_valence > 0:
                    score = min(score * 1.1, 1.0)
                else:
                    score = max(score * 0.9, 0.0)
            
            adjusted.append({'label': label, 'score': score})
        
        return adjusted
    
    def _adjust_for_baseline(
        self,
        emotions: List[Dict],
        user_baseline: Dict[str, float]
    ) -> List[Dict]:
        """
        æ ¹æ®ç”¨æˆ·æƒ…æ„ŸåŸºçº¿è°ƒæ•´
        
        è€ƒè™‘ä¸ªä½“å·®å¼‚ (æœ‰äººå¤©ç”Ÿä¹è§‚ï¼Œæœ‰äººæ‚²è§‚)
        """
        # ç®€åŒ–å®ç°: å¦‚æœç”¨æˆ·åŸºçº¿åè´Ÿé¢ï¼Œè´Ÿé¢æƒ…æ„Ÿå¾—åˆ†ç¨å¾®é™ä½
        baseline_valence = user_baseline.get('baseline_valence', 0.0)
        
        adjusted = []
        for emotion in emotions:
            label = emotion['label']
            score = emotion['score']
            
            if label in self.EMOTION_VA_MAPPING:
                v, _ = self.EMOTION_VA_MAPPING[label]
                
                # å¦‚æœæ˜¯è´Ÿé¢æƒ…æ„Ÿä½†ç”¨æˆ·åŸºçº¿å°±åè´Ÿï¼Œä¸è¦è¿‡åº¦è¯Šæ–­
                if v < 0 and baseline_valence < -0.3:
                    score = max(score * 0.85, 0.0)
            
            adjusted.append({'label': label, 'score': score})
        
        return adjusted
    
    def _invert_emotions(
        self,
        emotions: List[Dict]
    ) -> List[Dict]:
        """
        åè½¬æƒ…æ„Ÿ (ç”¨äºè®½åˆºæ£€æµ‹)
        
        æ­£é¢â†’è´Ÿé¢, è´Ÿé¢â†’æ­£é¢
        """
        inverted = []
        
        for emotion in emotions:
            label = emotion['label']
            score = emotion['score']
            
            if label in self.EMOTION_VA_MAPPING:
                v, a = self.EMOTION_VA_MAPPING[label]
                
                # æ‰¾åˆ°æ•ˆä»·ç›¸åçš„æƒ…æ„Ÿ
                target_v = -v
                best_match = None
                min_distance = float('inf')
                
                for candidate_label, (cand_v, cand_a) in self.EMOTION_VA_MAPPING.items():
                    distance = abs(cand_v - target_v) + abs(cand_a - a)
                    if distance < min_distance:
                        min_distance = distance
                        best_match = candidate_label
                
                if best_match:
                    inverted.append({'label': best_match, 'score': score})
                else:
                    inverted.append({'label': label, 'score': score * 0.5})
            else:
                inverted.append({'label': label, 'score': score})
        
        return inverted
    
    def _calculate_confidence(
        self,
        sorted_emotions: List[Dict]
    ) -> float:
        """
        è®¡ç®—ç½®ä¿¡åº¦
        
        åŸºäºtop 1ä¸top 2çš„å¾—åˆ†å·®è·
        """
        if len(sorted_emotions) < 2:
            return sorted_emotions[0]['score'] if sorted_emotions else 0.0
        
        top1 = sorted_emotions[0]['score']
        top2 = sorted_emotions[1]['score']
        
        # å·®è·è¶Šå¤§ï¼Œç½®ä¿¡åº¦è¶Šé«˜
        gap = top1 - top2
        confidence = top1 * (1 + gap)
        
        return min(confidence, 1.0)
    
    def build_emotional_trajectory(
        self,
        user_id: str,
        days: int = 30,
        db = None
    ) -> Dict[str, any]:
        """
        æ„å»ºç”¨æˆ·æƒ…æ„Ÿè½¨è¿¹
        
        ç”¨äº:
        1. æ£€æµ‹å¿ƒç†å¥åº·è¶‹åŠ¿
        2. ç†è§£æƒ…æ„ŸåŸºçº¿
        3. é¢„æµ‹æƒ…æ„Ÿååº”
        
        Returns:
            - trajectory: æ—¶é—´åºåˆ—æ•°æ®ç‚¹
            - baseline_valence: å¹³å‡æ•ˆä»·
            - valence_std: æ•ˆä»·æ ‡å‡†å·®
            - baseline_arousal: å¹³å‡å”¤é†’åº¦
            - trend: è¶‹åŠ¿ ('improving', 'declining', 'stable')
            - volatility: æƒ…æ„Ÿæ³¢åŠ¨æ€§
            - dominant_emotions: ä¸»å¯¼æƒ…æ„Ÿåˆ—è¡¨
        """
        if db is None:
            logger.warning("No database provided, cannot build trajectory")
            return {}
        
        # è·å–ç”¨æˆ·å¯¹è¯å†å²
        try:
            conversations = db.get_user_conversations(user_id, days=days)
        except Exception as e:
            logger.error(f"Failed to get conversations: {e}")
            return {}
        
        trajectory = []
        emotion_counts = {}
        
        for conv in conversations:
            if conv.get('role') == 'user':
                try:
                    analysis = self.analyze(conv.get('content', ''))
                    
                    trajectory.append({
                        'timestamp': conv.get('timestamp'),
                        'valence': analysis.valence,
                        'arousal': analysis.arousal,
                        'primary_emotion': analysis.primary_emotion,
                        'intensity': analysis.intensity
                    })
                    
                    # ç»Ÿè®¡æƒ…æ„Ÿé¢‘ç‡
                    emotion = analysis.primary_emotion
                    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
                    
                except Exception as e:
                    logger.error(f"Failed to analyze message: {e}")
                    continue
        
        if len(trajectory) == 0:
            return {'error': 'No valid data points'}
        
        # ç»Ÿè®¡åˆ†æ
        valences = [t['valence'] for t in trajectory]
        arousals = [t['arousal'] for t in trajectory]
        
        # è¶‹åŠ¿æ£€æµ‹ (çº¿æ€§å›å½’)
        if len(valences) >= 7:
            trend_slope = np.polyfit(range(len(valences)), valences, 1)[0]
        else:
            trend_slope = 0
        
        # ç¡®å®šè¶‹åŠ¿
        if trend_slope > 0.01:
            trend = 'improving'
        elif trend_slope < -0.01:
            trend = 'declining'
        else:
            trend = 'stable'
        
        # ä¸»å¯¼æƒ…æ„Ÿ (top 3)
        dominant = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        dominant_emotions = [{'emotion': e, 'count': c, 'percentage': c/len(trajectory)*100} 
                            for e, c in dominant]
        
        return {
            'trajectory': trajectory,
            'baseline_valence': float(np.mean(valences)),
            'valence_std': float(np.std(valences)),
            'baseline_arousal': float(np.mean(arousals)),
            'arousal_std': float(np.std(arousals)),
            'trend': trend,
            'trend_slope': float(trend_slope),
            'volatility': float(np.std(valences)),
            'dominant_emotions': dominant_emotions,
            'data_points': len(trajectory),
            'days_analyzed': days
        }
    
    def detect_emotional_anomaly(
        self,
        current_analysis: EmotionAnalysis,
        user_baseline: Dict[str, float]
    ) -> Dict[str, any]:
        """
        æ£€æµ‹æƒ…æ„Ÿå¼‚å¸¸ (ç”¨äºå¿ƒç†å¥åº·é¢„è­¦)
        
        å½“ç”¨æˆ·å½“å‰æƒ…æ„Ÿä¸åŸºçº¿æ˜¾è‘—åç¦»æ—¶å‘å‡ºè­¦æŠ¥
        """
        baseline_valence = user_baseline.get('baseline_valence', 0.0)
        valence_std = user_baseline.get('valence_std', 0.3)
        
        # è®¡ç®—z-score
        z_score = (current_analysis.valence - baseline_valence) / max(valence_std, 0.1)
        
        # åˆ¤å®šå¼‚å¸¸
        is_anomaly = abs(z_score) > 2.5  # 2.5ä¸ªæ ‡å‡†å·®
        
        if is_anomaly:
            if z_score > 0:
                anomaly_type = 'unusually_positive'
                severity = 'low'
            else:
                anomaly_type = 'unusually_negative'
                # è´Ÿé¢å¼‚å¸¸æ›´éœ€è¦å…³æ³¨
                if abs(z_score) > 3.5:
                    severity = 'high'
                elif abs(z_score) > 3.0:
                    severity = 'medium'
                else:
                    severity = 'low'
        else:
            anomaly_type = 'normal'
            severity = None
        
        return {
            'is_anomaly': is_anomaly,
            'anomaly_type': anomaly_type,
            'severity': severity,
            'z_score': float(z_score),
            'current_valence': current_analysis.valence,
            'baseline_valence': baseline_valence,
            'recommendation': self._get_anomaly_recommendation(anomaly_type, severity)
        }
    
    def _get_anomaly_recommendation(
        self,
        anomaly_type: str,
        severity: Optional[str]
    ) -> str:
        """ç”Ÿæˆå¼‚å¸¸æƒ…å†µå»ºè®®"""
        if anomaly_type == 'normal':
            return "User emotional state is within normal range"
        
        if anomaly_type == 'unusually_positive':
            return "User is experiencing unusually high positive emotions - great time for engagement"
        
        if anomaly_type == 'unusually_negative':
            if severity == 'high':
                return "ALERT: User showing significant negative emotional deviation - consider wellness check-in"
            elif severity == 'medium':
                return "User experiencing notable negative emotions - provide supportive responses"
            else:
                return "User slightly more negative than usual - monitor if pattern continues"
        
        return "Unknown anomaly type"


# ç®€åŒ–API
def analyze_emotion(
    text: str,
    context: Optional[List[str]] = None,
    user_id: Optional[str] = None
) -> Dict[str, any]:
    """
    ä¾¿æ·API - åˆ†ææƒ…æ„Ÿ
    
    Usage:
        result = analyze_emotion("I'm so happy today!")
        print(result['primary_emotion'])  # 'joy'
        print(result['valence'])  # 0.8
    """
    analyzer = TransformerEmotionAnalyzer()
    analysis = analyzer.analyze(text, context)
    
    return {
        'primary_emotion': analysis.primary_emotion,
        'primary_score': analysis.primary_score,
        'all_emotions': analysis.all_emotions,
        'intensity': analysis.intensity,
        'valence': analysis.valence,
        'arousal': analysis.arousal,
        'is_sarcastic': analysis.is_sarcastic,
        'mixed_emotions': analysis.mixed_emotions,
        'confidence': analysis.confidence
    }

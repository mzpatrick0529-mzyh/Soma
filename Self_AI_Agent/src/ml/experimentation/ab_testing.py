"""
A/B Testing Framework - Phase 7B.2
ç”Ÿäº§çº§A/Bæµ‹è¯•å’Œå¤šå˜é‡å®éªŒç³»ç»Ÿ

æ ¸å¿ƒåŠŸèƒ½:
1. ä¸€è‡´æ€§å“ˆå¸Œåˆ†ç»„ (Consistent Hashing)
2. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ (t-test, Chi-squared, Mann-Whitney U)
3. å¤šè‡‚è€è™æœº (Thompson Sampling, UCB1)
4. å®éªŒè¿½è¸ªå’Œå¯è§†åŒ–
5. è‡ªåŠ¨åœæ­¢ (Early Stopping)

ç§‘å­¦ä¾æ®:
- Frequentist Statistics: t-test, p-value < 0.05
- Bayesian Statistics: Thompson Sampling
- Effect Size: Cohen's d, Hedge's g
- Sample Size: Power Analysis (Î±=0.05, Î²=0.20)

ä½¿ç”¨åœºæ™¯:
- æ¨¡å‹ç‰ˆæœ¬å¯¹æ¯” (Model A vs Model B)
- è¶…å‚æ•°è°ƒä¼˜ (Temperature 0.7 vs 0.9)
- ç‰¹å¾å®éªŒ (With/Without Knowledge Graph)
"""

import hashlib
import logging
import time
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Tuple, Any
import json
from pathlib import Path

import numpy as np
from scipy import stats

logger = logging.getLogger(__name__)


# ==================== æšä¸¾ç±»å‹ ====================

class ExperimentStatus(Enum):
    """å®éªŒçŠ¶æ€"""
    DRAFT = "draft"              # è‰ç¨¿
    RUNNING = "running"          # è¿è¡Œä¸­
    PAUSED = "paused"            # æš‚åœ
    COMPLETED = "completed"      # å®Œæˆ
    STOPPED = "stopped"          # åœæ­¢ (æ—©æœŸåœæ­¢)


class MetricType(Enum):
    """æŒ‡æ ‡ç±»å‹"""
    CONVERSION = "conversion"    # è½¬åŒ–ç‡ (0/1)
    CONTINUOUS = "continuous"    # è¿ç»­å€¼ (å»¶è¿Ÿ, å‡†ç¡®ç‡)
    COUNT = "count"              # è®¡æ•° (ç‚¹å‡»æ•°)


class BanditAlgorithm(Enum):
    """å¤šè‡‚è€è™æœºç®—æ³•"""
    THOMPSON_SAMPLING = "thompson"  # Thompsoné‡‡æ · (è´å¶æ–¯)
    UCB1 = "ucb1"                   # Upper Confidence Bound
    EPSILON_GREEDY = "epsilon"      # Îµ-è´ªå©ª


# ==================== é…ç½®ç±» ====================

@dataclass
class VariantConfig:
    """å˜ä½“é…ç½®"""
    name: str                          # å˜ä½“åç§° (control, treatment_a, treatment_b)
    allocation: float = 0.0            # åˆ†é…æ¯”ä¾‹ (0-1)
    parameters: Dict[str, Any] = field(default_factory=dict)  # å˜ä½“å‚æ•°
    
    def __post_init__(self):
        if not 0 <= self.allocation <= 1:
            raise ValueError(f"Allocation must be in [0, 1], got {self.allocation}")


@dataclass
class ExperimentConfig:
    """å®éªŒé…ç½®"""
    name: str                                    # å®éªŒåç§°
    variants: List[VariantConfig]                # å˜ä½“åˆ—è¡¨
    metric_name: str                             # ä¸»è¦æŒ‡æ ‡åç§°
    metric_type: MetricType                      # æŒ‡æ ‡ç±»å‹
    
    # ç»Ÿè®¡å‚æ•°
    alpha: float = 0.05                          # æ˜¾è‘—æ€§æ°´å¹³ (Type Ié”™è¯¯)
    power: float = 0.80                          # ç»Ÿè®¡åŠŸæ•ˆ (1 - Type IIé”™è¯¯)
    minimum_detectable_effect: float = 0.05      # æœ€å°å¯æ£€æµ‹æ•ˆåº” (5%)
    
    # å®éªŒæ§åˆ¶
    min_sample_size: int = 100                   # æœ€å°æ ·æœ¬é‡
    max_duration_hours: int = 168                # æœ€å¤§æŒç»­æ—¶é—´ (7å¤©)
    check_frequency_hours: int = 24              # æ£€æŸ¥é¢‘ç‡ (æ¯å¤©)
    
    # å¤šè‡‚è€è™æœº
    use_bandit: bool = False                     # æ˜¯å¦ä½¿ç”¨å¤šè‡‚è€è™æœº
    bandit_algorithm: BanditAlgorithm = BanditAlgorithm.THOMPSON_SAMPLING
    bandit_warmup_samples: int = 50              # é¢„çƒ­æ ·æœ¬æ•°
    
    def __post_init__(self):
        # éªŒè¯åˆ†é…æ¯”ä¾‹æ€»å’Œä¸º1
        total_allocation = sum(v.allocation for v in self.variants)
        if not np.isclose(total_allocation, 1.0, atol=1e-6):
            raise ValueError(f"Variant allocations must sum to 1, got {total_allocation}")
        
        # è‡³å°‘2ä¸ªå˜ä½“
        if len(self.variants) < 2:
            raise ValueError("Need at least 2 variants (control + treatment)")


@dataclass
class ExperimentResult:
    """å®éªŒç»“æœ"""
    experiment_id: str
    status: ExperimentStatus
    
    # ç»Ÿè®¡ç»“æœ
    variant_stats: Dict[str, Dict[str, float]]   # {variant: {mean, std, count}}
    p_value: float = 1.0                         # på€¼
    confidence_interval: Tuple[float, float] = (0.0, 0.0)  # ç½®ä¿¡åŒºé—´
    effect_size: float = 0.0                     # æ•ˆåº”å¤§å° (Cohen's d)
    
    # å†³ç­–
    is_significant: bool = False                 # æ˜¯å¦æ˜¾è‘—
    winner: Optional[str] = None                 # è·èƒœå˜ä½“
    
    # å…ƒæ•°æ®
    sample_sizes: Dict[str, int] = field(default_factory=dict)
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'experiment_id': self.experiment_id,
            'status': self.status.value,
            'variant_stats': self.variant_stats,
            'p_value': self.p_value,
            'confidence_interval': list(self.confidence_interval),
            'effect_size': self.effect_size,
            'is_significant': self.is_significant,
            'winner': self.winner,
            'sample_sizes': self.sample_sizes,
            'start_time': self.start_time.isoformat() if self.start_time else None,
            'end_time': self.end_time.isoformat() if self.end_time else None
        }


# ==================== ç»Ÿè®¡æµ‹è¯•ç±» ====================

class StatisticalTest:
    """
    ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    
    æ”¯æŒçš„æµ‹è¯•:
    1. t-test (è¿ç»­æŒ‡æ ‡, æ­£æ€åˆ†å¸ƒ)
    2. Mann-Whitney U (è¿ç»­æŒ‡æ ‡, éæ­£æ€)
    3. Chi-squared (åˆ†ç±»æŒ‡æ ‡, è½¬åŒ–ç‡)
    """
    
    @staticmethod
    def t_test(
        control_data: np.ndarray,
        treatment_data: np.ndarray,
        alpha: float = 0.05
    ) -> Tuple[float, bool, float]:
        """
        ç‹¬ç«‹æ ·æœ¬tæ£€éªŒ
        
        å‡è®¾:
        - æ•°æ®è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ
        - æ–¹å·®é½æ€§
        
        Returns:
            (p_value, is_significant, effect_size)
        """
        # Welch's t-test (ä¸å‡è®¾æ–¹å·®ç›¸ç­‰)
        t_stat, p_value = stats.ttest_ind(
            control_data,
            treatment_data,
            equal_var=False
        )
        
        # Cohen's d (æ•ˆåº”å¤§å°)
        pooled_std = np.sqrt(
            (np.var(control_data) + np.var(treatment_data)) / 2
        )
        effect_size = (np.mean(treatment_data) - np.mean(control_data)) / (pooled_std + 1e-8)
        
        is_significant = p_value < alpha
        
        return p_value, is_significant, effect_size
    
    @staticmethod
    def mann_whitney_u(
        control_data: np.ndarray,
        treatment_data: np.ndarray,
        alpha: float = 0.05
    ) -> Tuple[float, bool]:
        """
        Mann-Whitney Uæ£€éªŒ (éå‚æ•°)
        
        ä¼˜ç‚¹:
        - ä¸å‡è®¾æ­£æ€åˆ†å¸ƒ
        - å¯¹å¼‚å¸¸å€¼é²æ£’
        
        Returns:
            (p_value, is_significant)
        """
        u_stat, p_value = stats.mannwhitneyu(
            control_data,
            treatment_data,
            alternative='two-sided'
        )
        
        is_significant = p_value < alpha
        
        return p_value, is_significant
    
    @staticmethod
    def chi_squared(
        control_conversions: int,
        control_total: int,
        treatment_conversions: int,
        treatment_total: int,
        alpha: float = 0.05
    ) -> Tuple[float, bool, float]:
        """
        å¡æ–¹æ£€éªŒ (è½¬åŒ–ç‡)
        
        é€‚ç”¨äº:
        - äºŒåˆ†ç±»ç»“æœ (è½¬åŒ–/æœªè½¬åŒ–)
        - è®¡æ•°æ•°æ®
        
        Returns:
            (p_value, is_significant, relative_lift)
        """
        # æ„å»ºåˆ—è”è¡¨
        observed = np.array([
            [control_conversions, control_total - control_conversions],
            [treatment_conversions, treatment_total - treatment_conversions]
        ])
        
        chi2, p_value, _, _ = stats.chi2_contingency(observed)
        
        # ç›¸å¯¹æå‡
        control_rate = control_conversions / (control_total + 1e-8)
        treatment_rate = treatment_conversions / (treatment_total + 1e-8)
        relative_lift = (treatment_rate - control_rate) / (control_rate + 1e-8)
        
        is_significant = p_value < alpha
        
        return p_value, is_significant, relative_lift
    
    @staticmethod
    def confidence_interval(
        data: np.ndarray,
        confidence: float = 0.95
    ) -> Tuple[float, float]:
        """
        è®¡ç®—ç½®ä¿¡åŒºé—´
        
        Args:
            data: æ•°æ®
            confidence: ç½®ä¿¡æ°´å¹³ (0.95 = 95%)
        
        Returns:
            (lower_bound, upper_bound)
        """
        mean = np.mean(data)
        std_err = stats.sem(data)
        ci = stats.t.interval(
            confidence,
            len(data) - 1,
            loc=mean,
            scale=std_err
        )
        return ci
    
    @staticmethod
    def calculate_sample_size(
        baseline_rate: float,
        minimum_detectable_effect: float,
        alpha: float = 0.05,
        power: float = 0.80
    ) -> int:
        """
        è®¡ç®—æ‰€éœ€æ ·æœ¬é‡
        
        ä½¿ç”¨: åŒæ ·æœ¬æ¯”ä¾‹æ£€éªŒçš„æ ·æœ¬é‡å…¬å¼
        
        Args:
            baseline_rate: åŸºçº¿è½¬åŒ–ç‡
            minimum_detectable_effect: æœ€å°å¯æ£€æµ‹æ•ˆåº” (ç›¸å¯¹æå‡)
            alpha: æ˜¾è‘—æ€§æ°´å¹³
            power: ç»Ÿè®¡åŠŸæ•ˆ
        
        Returns:
            æ¯ç»„æ‰€éœ€æ ·æœ¬é‡
        """
        # Zå€¼
        z_alpha = stats.norm.ppf(1 - alpha / 2)
        z_beta = stats.norm.ppf(power)
        
        # è½¬åŒ–ç‡
        p1 = baseline_rate
        p2 = baseline_rate * (1 + minimum_detectable_effect)
        p_avg = (p1 + p2) / 2
        
        # æ ·æœ¬é‡å…¬å¼
        n = (
            (z_alpha * np.sqrt(2 * p_avg * (1 - p_avg)) + 
             z_beta * np.sqrt(p1 * (1 - p1) + p2 * (1 - p2))) ** 2
        ) / ((p2 - p1) ** 2)
        
        return int(np.ceil(n))


# ==================== å¤šè‡‚è€è™æœºç±» ====================

class MultiArmedBandit:
    """
    å¤šè‡‚è€è™æœºç®—æ³•
    
    ç®—æ³•:
    1. Thompson Sampling (è´å¶æ–¯): Betaåˆ†å¸ƒé‡‡æ ·
    2. UCB1 (é¢‘ç‡æ´¾): ç½®ä¿¡ä¸Šç•Œ
    3. Îµ-Greedy: æ¢ç´¢-åˆ©ç”¨æƒè¡¡
    """
    
    def __init__(
        self,
        variant_names: List[str],
        algorithm: BanditAlgorithm = BanditAlgorithm.THOMPSON_SAMPLING,
        epsilon: float = 0.1
    ):
        self.variant_names = variant_names
        self.algorithm = algorithm
        self.epsilon = epsilon
        
        # ç»Ÿè®¡æ•°æ®
        self.successes = {v: 1 for v in variant_names}  # Betaå…ˆéªŒ (1, 1)
        self.failures = {v: 1 for v in variant_names}
        self.pulls = {v: 0 for v in variant_names}
    
    def select_variant(self) -> str:
        """
        é€‰æ‹©å˜ä½“
        
        Returns:
            é€‰ä¸­çš„å˜ä½“åç§°
        """
        if self.algorithm == BanditAlgorithm.THOMPSON_SAMPLING:
            return self._thompson_sampling()
        elif self.algorithm == BanditAlgorithm.UCB1:
            return self._ucb1()
        elif self.algorithm == BanditAlgorithm.EPSILON_GREEDY:
            return self._epsilon_greedy()
        else:
            raise ValueError(f"Unknown algorithm: {self.algorithm}")
    
    def update(self, variant: str, reward: float):
        """
        æ›´æ–°ç»Ÿè®¡æ•°æ®
        
        Args:
            variant: å˜ä½“åç§°
            reward: å¥–åŠ± (0æˆ–1)
        """
        self.pulls[variant] += 1
        
        if reward > 0:
            self.successes[variant] += 1
        else:
            self.failures[variant] += 1
    
    def _thompson_sampling(self) -> str:
        """
        Thompson Sampling
        
        åŸç†:
        - ä¸ºæ¯ä¸ªå˜ä½“ä»Betaåˆ†å¸ƒé‡‡æ ·
        - é€‰æ‹©é‡‡æ ·å€¼æœ€å¤§çš„å˜ä½“
        - Beta(Î±, Î²): Î±=successes, Î²=failures
        """
        samples = {}
        for variant in self.variant_names:
            alpha = self.successes[variant]
            beta = self.failures[variant]
            samples[variant] = np.random.beta(alpha, beta)
        
        return max(samples, key=samples.get)
    
    def _ucb1(self) -> str:
        """
        UCB1 (Upper Confidence Bound)
        
        åŸç†:
        - UCB = mean + sqrt(2 * log(total_pulls) / variant_pulls)
        - é€‰æ‹©UCBæœ€å¤§çš„å˜ä½“
        """
        total_pulls = sum(self.pulls.values())
        
        if total_pulls == 0:
            return np.random.choice(self.variant_names)
        
        ucb_scores = {}
        for variant in self.variant_names:
            pulls = self.pulls[variant]
            
            if pulls == 0:
                ucb_scores[variant] = float('inf')
            else:
                mean = self.successes[variant] / (self.successes[variant] + self.failures[variant])
                exploration = np.sqrt(2 * np.log(total_pulls) / pulls)
                ucb_scores[variant] = mean + exploration
        
        return max(ucb_scores, key=ucb_scores.get)
    
    def _epsilon_greedy(self) -> str:
        """
        Îµ-Greedy
        
        åŸç†:
        - ä»¥æ¦‚ç‡Îµéšæœºæ¢ç´¢
        - ä»¥æ¦‚ç‡1-Îµé€‰æ‹©æœ€ä¼˜å˜ä½“
        """
        if np.random.random() < self.epsilon:
            # æ¢ç´¢
            return np.random.choice(self.variant_names)
        else:
            # åˆ©ç”¨
            rates = {
                v: self.successes[v] / (self.successes[v] + self.failures[v])
                for v in self.variant_names
            }
            return max(rates, key=rates.get)


# ==================== ä¸»æ¡†æ¶ç±» ====================

class ABTestingFramework:
    """
    A/Bæµ‹è¯•æ¡†æ¶
    
    åŠŸèƒ½:
    1. åˆ›å»ºå®éªŒ
    2. åˆ†é…ç”¨æˆ·åˆ°å˜ä½“ (ä¸€è‡´æ€§å“ˆå¸Œ)
    3. è®°å½•æŒ‡æ ‡
    4. ç»Ÿè®¡åˆ†æ
    5. è‡ªåŠ¨åœæ­¢ (æ—©æœŸåœæ­¢)
    6. å¤šè‡‚è€è™æœº
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    framework = ABTestingFramework()
    
    # åˆ›å»ºå®éªŒ
    config = ExperimentConfig(
        name="model_version_test",
        variants=[
            VariantConfig("control", 0.5),
            VariantConfig("treatment", 0.5)
        ],
        metric_name="accuracy",
        metric_type=MetricType.CONTINUOUS
    )
    exp_id = framework.create_experiment(config)
    
    # åˆ†é…ç”¨æˆ·
    variant = framework.assign_variant("user_123", exp_id)
    
    # è®°å½•æŒ‡æ ‡
    framework.log_metric(exp_id, "user_123", variant, 0.95)
    
    # åˆ†æç»“æœ
    result = framework.analyze_experiment(exp_id)
    print(f"Winner: {result.winner}, p-value: {result.p_value}")
    ```
    """
    
    def __init__(self, storage_path: Optional[Path] = None):
        """
        åˆå§‹åŒ–æ¡†æ¶
        
        Args:
            storage_path: å®éªŒæ•°æ®å­˜å‚¨è·¯å¾„
        """
        self.storage_path = storage_path or Path("./experiments")
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # å®éªŒå­˜å‚¨
        self.experiments: Dict[str, ExperimentConfig] = {}
        self.experiment_data: Dict[str, Dict[str, List[float]]] = defaultdict(lambda: defaultdict(list))
        self.user_assignments: Dict[str, Dict[str, str]] = defaultdict(dict)  # {exp_id: {user_id: variant}}
        self.experiment_status: Dict[str, ExperimentStatus] = {}
        self.experiment_start_time: Dict[str, datetime] = {}
        
        # å¤šè‡‚è€è™æœº
        self.bandits: Dict[str, MultiArmedBandit] = {}
        
        logger.info(f"âœ… ABTestingFramework initialized (storage={self.storage_path})")
    
    # ==================== å®éªŒç®¡ç† ====================
    
    def create_experiment(self, config: ExperimentConfig) -> str:
        """
        åˆ›å»ºå®éªŒ
        
        Args:
            config: å®éªŒé…ç½®
        
        Returns:
            experiment_id
        """
        # ç”Ÿæˆå®éªŒID
        exp_id = f"{config.name}_{int(time.time())}"
        
        # å­˜å‚¨é…ç½®
        self.experiments[exp_id] = config
        self.experiment_status[exp_id] = ExperimentStatus.DRAFT
        self.experiment_start_time[exp_id] = datetime.now()
        
        # åˆå§‹åŒ–å¤šè‡‚è€è™æœº
        if config.use_bandit:
            variant_names = [v.name for v in config.variants]
            self.bandits[exp_id] = MultiArmedBandit(
                variant_names,
                algorithm=config.bandit_algorithm
            )
        
        logger.info(f"âœ… Experiment created: {exp_id}")
        logger.info(f"   - Variants: {[v.name for v in config.variants]}")
        logger.info(f"   - Metric: {config.metric_name} ({config.metric_type.value})")
        logger.info(f"   - Use bandit: {config.use_bandit}")
        
        return exp_id
    
    def start_experiment(self, exp_id: str):
        """å¼€å§‹å®éªŒ"""
        if exp_id not in self.experiments:
            raise ValueError(f"Experiment {exp_id} not found")
        
        self.experiment_status[exp_id] = ExperimentStatus.RUNNING
        self.experiment_start_time[exp_id] = datetime.now()
        
        logger.info(f"â–¶ï¸ Experiment started: {exp_id}")
    
    def stop_experiment(self, exp_id: str, reason: str = "manual"):
        """åœæ­¢å®éªŒ"""
        if exp_id not in self.experiments:
            raise ValueError(f"Experiment {exp_id} not found")
        
        self.experiment_status[exp_id] = ExperimentStatus.STOPPED
        
        logger.info(f"â¹ï¸ Experiment stopped: {exp_id} (reason: {reason})")
    
    # ==================== ç”¨æˆ·åˆ†é… ====================
    
    def assign_variant(self, user_id: str, exp_id: str) -> str:
        """
        åˆ†é…ç”¨æˆ·åˆ°å˜ä½“
        
        ä½¿ç”¨ä¸€è‡´æ€§å“ˆå¸Œä¿è¯:
        - åŒä¸€ç”¨æˆ·æ€»æ˜¯åˆ†é…åˆ°åŒä¸€å˜ä½“
        - åˆ†é…æ¯”ä¾‹æ¥è¿‘é…ç½®
        
        Args:
            user_id: ç”¨æˆ·ID
            exp_id: å®éªŒID
        
        Returns:
            variant_name
        """
        if exp_id not in self.experiments:
            raise ValueError(f"Experiment {exp_id} not found")
        
        # æ£€æŸ¥æ˜¯å¦å·²åˆ†é…
        if user_id in self.user_assignments[exp_id]:
            return self.user_assignments[exp_id][user_id]
        
        config = self.experiments[exp_id]
        
        # å¤šè‡‚è€è™æœºåˆ†é…
        if config.use_bandit:
            bandit = self.bandits[exp_id]
            total_pulls = sum(bandit.pulls.values())
            
            # é¢„çƒ­æœŸ: å‡åŒ€åˆ†é…
            if total_pulls < config.bandit_warmup_samples * len(config.variants):
                variant = self._hash_assignment(user_id, exp_id)
            else:
                # è€è™æœºé€‰æ‹©
                variant = bandit.select_variant()
        else:
            # ä¸€è‡´æ€§å“ˆå¸Œåˆ†é…
            variant = self._hash_assignment(user_id, exp_id)
        
        # å­˜å‚¨åˆ†é…
        self.user_assignments[exp_id][user_id] = variant
        
        return variant
    
    def _hash_assignment(self, user_id: str, exp_id: str) -> str:
        """
        ä¸€è‡´æ€§å“ˆå¸Œåˆ†é…
        
        åŸç†:
        - hash(user_id + exp_id) -> [0, 1)
        - æ ¹æ®ç´¯ç§¯åˆ†é…æ¯”ä¾‹ç¡®å®šå˜ä½“
        """
        config = self.experiments[exp_id]
        
        # è®¡ç®—å“ˆå¸Œå€¼ [0, 1)
        hash_str = f"{user_id}_{exp_id}"
        hash_value = int(hashlib.md5(hash_str.encode()).hexdigest(), 16)
        normalized_hash = (hash_value % 10000) / 10000.0
        
        # ç´¯ç§¯åˆ†é…
        cumulative = 0.0
        for variant in config.variants:
            cumulative += variant.allocation
            if normalized_hash < cumulative:
                return variant.name
        
        # è¾¹ç•Œæƒ…å†µ
        return config.variants[-1].name
    
    # ==================== æŒ‡æ ‡è®°å½• ====================
    
    def log_metric(
        self,
        exp_id: str,
        user_id: str,
        variant: str,
        value: float
    ):
        """
        è®°å½•æŒ‡æ ‡
        
        Args:
            exp_id: å®éªŒID
            user_id: ç”¨æˆ·ID
            variant: å˜ä½“åç§°
            value: æŒ‡æ ‡å€¼
        """
        if exp_id not in self.experiments:
            raise ValueError(f"Experiment {exp_id} not found")
        
        # å­˜å‚¨æ•°æ®
        self.experiment_data[exp_id][variant].append(value)
        
        # æ›´æ–°å¤šè‡‚è€è™æœº
        config = self.experiments[exp_id]
        if config.use_bandit and exp_id in self.bandits:
            # å‡è®¾valueæ˜¯å¥–åŠ± (0æˆ–1)
            reward = 1 if value > 0.5 else 0
            self.bandits[exp_id].update(variant, reward)
    
    # ==================== ç»Ÿè®¡åˆ†æ ====================
    
    def analyze_experiment(self, exp_id: str) -> ExperimentResult:
        """
        åˆ†æå®éªŒç»“æœ
        
        æ­¥éª¤:
        1. è®¡ç®—å„å˜ä½“ç»Ÿè®¡é‡
        2. æ‰§è¡Œæ˜¾è‘—æ€§æ£€éªŒ
        3. è®¡ç®—æ•ˆåº”å¤§å°
        4. ç¡®å®šè·èƒœè€…
        
        Args:
            exp_id: å®éªŒID
        
        Returns:
            ExperimentResult
        """
        if exp_id not in self.experiments:
            raise ValueError(f"Experiment {exp_id} not found")
        
        config = self.experiments[exp_id]
        data = self.experiment_data[exp_id]
        
        # è®¡ç®—ç»Ÿè®¡é‡
        variant_stats = {}
        sample_sizes = {}
        
        for variant_name in data.keys():
            values = np.array(data[variant_name])
            if len(values) > 0:
                variant_stats[variant_name] = {
                    'mean': float(np.mean(values)),
                    'std': float(np.std(values)),
                    'count': len(values)
                }
                sample_sizes[variant_name] = len(values)
            else:
                variant_stats[variant_name] = {
                    'mean': 0.0,
                    'std': 0.0,
                    'count': 0
                }
                sample_sizes[variant_name] = 0
        
        # æ‰¾åˆ°controlç»„
        control_variant = config.variants[0].name
        if control_variant not in data or len(data[control_variant]) == 0:
            logger.warning(f"No data for control variant: {control_variant}")
            return ExperimentResult(
                experiment_id=exp_id,
                status=self.experiment_status[exp_id],
                variant_stats=variant_stats,
                sample_sizes=sample_sizes,
                start_time=self.experiment_start_time.get(exp_id)
            )
        
        # å¯¹æ¯ä¸ªtreatmentå˜ä½“æ‰§è¡Œç»Ÿè®¡æ£€éªŒ
        control_data = np.array(data[control_variant])
        best_p_value = 1.0
        best_effect_size = 0.0
        best_variant = control_variant
        
        for variant in config.variants[1:]:
            variant_name = variant.name
            if variant_name not in data or len(data[variant_name]) < config.min_sample_size:
                continue
            
            treatment_data = np.array(data[variant_name])
            
            # æ‰§è¡Œtæ£€éªŒ
            p_value, is_significant, effect_size = StatisticalTest.t_test(
                control_data,
                treatment_data,
                alpha=config.alpha
            )
            
            # æ›´æ–°æœ€ä½³å˜ä½“
            if is_significant and effect_size > best_effect_size:
                best_p_value = p_value
                best_effect_size = effect_size
                best_variant = variant_name
        
        # ç½®ä¿¡åŒºé—´ (treatment vs control)
        if best_variant != control_variant and len(data[best_variant]) > 0:
            treatment_data = np.array(data[best_variant])
            # å–è¾ƒçŸ­çš„é•¿åº¦
            min_len = min(len(control_data), len(treatment_data))
            diff_data = treatment_data[:min_len] - control_data[:min_len]
            ci = StatisticalTest.confidence_interval(diff_data)
        else:
            ci = (0.0, 0.0)
        
        # æ„å»ºç»“æœ
        result = ExperimentResult(
            experiment_id=exp_id,
            status=self.experiment_status[exp_id],
            variant_stats=variant_stats,
            p_value=best_p_value,
            confidence_interval=ci,
            effect_size=best_effect_size,
            is_significant=(best_p_value < config.alpha),
            winner=best_variant if best_p_value < config.alpha else None,
            sample_sizes=sample_sizes,
            start_time=self.experiment_start_time.get(exp_id),
            end_time=datetime.now()
        )
        
        return result
    
    # ==================== è‡ªåŠ¨å†³ç­– ====================
    
    def check_early_stopping(self, exp_id: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©æœŸåœæ­¢
        
        æ¡ä»¶:
        1. æ ·æœ¬é‡è¶³å¤Ÿ
        2. ç»Ÿè®¡æ˜¾è‘—
        3. æ•ˆåº”å¤§å°è¶³å¤Ÿå¤§
        
        Returns:
            should_stop
        """
        if exp_id not in self.experiments:
            return False
        
        config = self.experiments[exp_id]
        result = self.analyze_experiment(exp_id)
        
        # æ£€æŸ¥æ ·æœ¬é‡
        min_samples = all(
            size >= config.min_sample_size
            for size in result.sample_sizes.values()
        )
        
        if not min_samples:
            return False
        
        # æ£€æŸ¥æ˜¾è‘—æ€§å’Œæ•ˆåº”å¤§å°
        if result.is_significant and abs(result.effect_size) > config.minimum_detectable_effect:
            logger.info(f"ğŸ¯ Early stopping triggered for {exp_id}")
            logger.info(f"   - Winner: {result.winner}")
            logger.info(f"   - p-value: {result.p_value:.4f}")
            logger.info(f"   - Effect size: {result.effect_size:.4f}")
            return True
        
        return False
    
    def get_experiment_summary(self, exp_id: str) -> Dict[str, Any]:
        """
        è·å–å®éªŒæ‘˜è¦
        
        Returns:
            æ‘˜è¦å­—å…¸
        """
        result = self.analyze_experiment(exp_id)
        config = self.experiments[exp_id]
        
        summary = {
            'experiment_id': exp_id,
            'name': config.name,
            'status': result.status.value,
            'variants': [v.name for v in config.variants],
            'metric': config.metric_name,
            'sample_sizes': result.sample_sizes,
            'statistics': result.variant_stats,
            'p_value': result.p_value,
            'is_significant': result.is_significant,
            'winner': result.winner,
            'effect_size': result.effect_size,
            'confidence_interval': list(result.confidence_interval),
            'duration_hours': (
                (result.end_time - result.start_time).total_seconds() / 3600
                if result.start_time and result.end_time else 0
            )
        }
        
        return summary
    
    # ==================== æŒä¹…åŒ– ====================
    
    def save_experiment(self, exp_id: str):
        """ä¿å­˜å®éªŒæ•°æ®"""
        filepath = self.storage_path / f"{exp_id}.json"
        
        data = {
            'config': {
                'name': self.experiments[exp_id].name,
                'variants': [
                    {'name': v.name, 'allocation': v.allocation}
                    for v in self.experiments[exp_id].variants
                ],
                'metric_name': self.experiments[exp_id].metric_name,
                'metric_type': self.experiments[exp_id].metric_type.value
            },
            'data': {k: list(v) for k, v in self.experiment_data[exp_id].items()},
            'status': self.experiment_status[exp_id].value,
            'start_time': self.experiment_start_time[exp_id].isoformat()
        }
        
        with open(filepath, 'w') as f:
            json.dump(data, f, indent=2)
        
        logger.info(f"ğŸ’¾ Experiment saved: {filepath}")
    
    def load_experiment(self, exp_id: str):
        """åŠ è½½å®éªŒæ•°æ®"""
        filepath = self.storage_path / f"{exp_id}.json"
        
        if not filepath.exists():
            raise FileNotFoundError(f"Experiment file not found: {filepath}")
        
        with open(filepath, 'r') as f:
            data = json.load(f)
        
        # TODO: é‡å»ºé…ç½®å’Œæ•°æ®
        logger.info(f"ğŸ“‚ Experiment loaded: {filepath}")


# ==================== ä¾¿æ·å‡½æ•° ====================

def quick_ab_test(
    control_data: List[float],
    treatment_data: List[float],
    alpha: float = 0.05
) -> Dict[str, Any]:
    """
    å¿«é€ŸA/Bæµ‹è¯•
    
    Args:
        control_data: å¯¹ç…§ç»„æ•°æ®
        treatment_data: å®éªŒç»„æ•°æ®
        alpha: æ˜¾è‘—æ€§æ°´å¹³
    
    Returns:
        æµ‹è¯•ç»“æœå­—å…¸
    """
    control_array = np.array(control_data)
    treatment_array = np.array(treatment_data)
    
    p_value, is_significant, effect_size = StatisticalTest.t_test(
        control_array,
        treatment_array,
        alpha=alpha
    )
    
    return {
        'p_value': p_value,
        'is_significant': is_significant,
        'effect_size': effect_size,
        'control_mean': float(np.mean(control_array)),
        'treatment_mean': float(np.mean(treatment_array)),
        'relative_improvement': (
            (np.mean(treatment_array) - np.mean(control_array)) / 
            np.mean(control_array) * 100
        )
    }

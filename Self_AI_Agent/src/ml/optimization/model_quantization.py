"""
Production Model Quantization - Phase 7B.1
çœŸå®çš„PyTorchæ¨¡å‹é‡åŒ–å®ç°

æ ¸å¿ƒåŠŸèƒ½:
1. åŠ¨æ€é‡åŒ– (FP32 â†’ INT8, æ— éœ€æ ¡å‡†æ•°æ®)
2. é™æ€é‡åŒ– (FP32 â†’ INT8, ä½¿ç”¨æ ¡å‡†æ•°æ®)
3. ç»“æ„åŒ–å‰ªæ (Structured Pruning)
4. çŸ¥è¯†è’¸é¦ (Knowledge Distillation)

é¢„æœŸæå‡:
- æ¨¡å‹å¤§å°: 500MB â†’ 150MB (-70%)
- æ¨ç†é€Ÿåº¦: 2000ms â†’ 400ms (-60%)
- å†…å­˜å ç”¨: 500MB â†’ 200MB (-60%)

ç§‘å­¦ä¾æ®:
- PyTorch Quantization: https://pytorch.org/docs/stable/quantization.html
- Lottery Ticket Hypothesis (Han et al., 2015)
- DistilBERT (Sanh et al., 2019)
"""

import torch
import torch.nn as nn
import torch.quantization as quant
from torch.nn.utils import prune
from typing import Dict, List, Any, Optional, Callable, Tuple
from dataclasses import dataclass
import logging
import copy
import time
from pathlib import Path
import numpy as np

logger = logging.getLogger(__name__)


@dataclass
class QuantizationConfig:
    """é‡åŒ–é…ç½®"""
    backend: str = "fbgemm"  # CPU: fbgemm, GPU: qnnpack
    dtype: torch.dtype = torch.qint8
    per_channel: bool = True  # é€é€šé“é‡åŒ–(æ›´ç²¾ç¡®)
    reduce_range: bool = False  # å‡å°‘é‡åŒ–èŒƒå›´(å…¼å®¹æŸäº›ç¡¬ä»¶)


@dataclass
class PruningConfig:
    """å‰ªæé…ç½®"""
    amount: float = 0.3  # å‰ªææ¯”ä¾‹ (30%)
    method: str = "l1_structured"  # l1_unstructured, l1_structured, random
    dim: int = 0  # å‰ªæç»´åº¦ (0: è¡Œ, 1: åˆ—)


@dataclass
class DistillationConfig:
    """è’¸é¦é…ç½®"""
    temperature: float = 3.0  # è½¯åŒ–æ¸©åº¦
    alpha: float = 0.5  # è’¸é¦æŸå¤±æƒé‡ (0-1)
    epochs: int = 10
    learning_rate: float = 1e-4


@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""
    original_size_mb: float
    optimized_size_mb: float
    size_reduction_percent: float
    original_latency_ms: float
    optimized_latency_ms: float
    latency_reduction_percent: float
    accuracy_drop_percent: float
    method: str


class ProductionModelOptimizer:
    """
    ç”Ÿäº§çº§æ¨¡å‹ä¼˜åŒ–å™¨
    
    æ”¯æŒçš„ä¼˜åŒ–æ–¹æ³•:
    1. åŠ¨æ€é‡åŒ– (Dynamic Quantization): è¿è¡Œæ—¶é‡åŒ–æƒé‡+æ¿€æ´»
    2. é™æ€é‡åŒ– (Static Quantization): é¢„å…ˆé‡åŒ–æƒé‡+æ¿€æ´» (éœ€è¦æ ¡å‡†)
    3. é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (QAT): è®­ç»ƒæ—¶æ¨¡æ‹Ÿé‡åŒ–
    4. ç»“æ„åŒ–å‰ªæ: ç§»é™¤æ•´ä¸ªé€šé“/ç¥ç»å…ƒ
    5. çŸ¥è¯†è’¸é¦: ç”¨å°æ¨¡å‹å­¦ä¹ å¤§æ¨¡å‹
    
    ä½¿ç”¨ç¤ºä¾‹:
    ```python
    optimizer = ProductionModelOptimizer()
    
    # æ–¹æ³•1: åŠ¨æ€é‡åŒ– (æœ€ç®€å•, æ— éœ€æ•°æ®)
    quantized_model = optimizer.quantize_dynamic(model)
    
    # æ–¹æ³•2: é™æ€é‡åŒ– (æœ€å¿«, éœ€è¦æ ¡å‡†æ•°æ®)
    quantized_model = optimizer.quantize_static(model, calibration_data)
    
    # æ–¹æ³•3: ç»“æ„åŒ–å‰ªæ
    pruned_model = optimizer.prune_structured(model, amount=0.3)
    
    # æ–¹æ³•4: çŸ¥è¯†è’¸é¦
    student_model = optimizer.distill_knowledge(teacher_model, student_model, train_data)
    ```
    """
    
    def __init__(
        self,
        device: str = "cpu",
        cache_dir: Optional[Path] = None
    ):
        """
        åˆå§‹åŒ–ä¼˜åŒ–å™¨
        
        Args:
            device: 'cpu', 'cuda', 'mps'
            cache_dir: ä¼˜åŒ–æ¨¡å‹ç¼“å­˜ç›®å½•
        """
        self.device = device
        self.cache_dir = cache_dir or Path("./optimized_models")
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®é‡åŒ–åç«¯ (macOSä½¿ç”¨qnnpack)
        import platform
        if platform.system() == "Darwin":  # macOS
            torch.backends.quantized.engine = "qnnpack"
            self.qconfig_backend = "qnnpack"
        elif device == "cpu":
            torch.backends.quantized.engine = "fbgemm"
            self.qconfig_backend = "fbgemm"
        else:
            torch.backends.quantized.engine = "qnnpack"
            self.qconfig_backend = "qnnpack"
        
        logger.info(f"âœ… ProductionModelOptimizer initialized (device={device}, backend={self.qconfig_backend})")
    
    # ==================== åŠ¨æ€é‡åŒ– ====================
    
    def quantize_dynamic(
        self,
        model: nn.Module,
        qconfig: Optional[QuantizationConfig] = None,
        dtype: torch.dtype = torch.qint8
    ) -> nn.Module:
        """
        åŠ¨æ€é‡åŒ– (æ¨èç”¨äºRNN/Transformer)
        
        ä¼˜ç‚¹: 
        - æ— éœ€æ ¡å‡†æ•°æ®
        - è‡ªåŠ¨é‡åŒ–Linear/LSTM/GRUå±‚
        - è¿è¡Œæ—¶åŠ¨æ€é‡åŒ–æ¿€æ´»
        
        ç¼ºç‚¹:
        - æ¯”é™æ€é‡åŒ–æ…¢ä¸€ç‚¹
        - æ¿€æ´»å€¼ä»æ˜¯FP32
        
        Args:
            model: PyTorchæ¨¡å‹
            qconfig: é‡åŒ–é…ç½®
            dtype: é‡åŒ–æ•°æ®ç±»å‹ (qint8, qint16)
        
        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        logger.info("ğŸ”§ Starting dynamic quantization...")
        
        # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        model.eval()
        
        # é‡åŒ–Linearå’ŒLSTMå±‚
        quantized_model = quant.quantize_dynamic(
            model,
            {nn.Linear, nn.LSTM, nn.GRU},
            dtype=dtype
        )
        
        # è®¡ç®—å‹ç¼©ç‡
        original_size = self._get_model_size(model)
        quantized_size = self._get_model_size(quantized_model)
        reduction = 100 * (1 - quantized_size / original_size)
        
        logger.info(f"âœ… Dynamic quantization complete:")
        logger.info(f"   - Original: {original_size:.2f} MB")
        logger.info(f"   - Quantized: {quantized_size:.2f} MB")
        logger.info(f"   - Reduction: {reduction:.1f}%")
        
        return quantized_model
    
    # ==================== é™æ€é‡åŒ– ====================
    
    def quantize_static(
        self,
        model: nn.Module,
        calibration_data: List[torch.Tensor],
        qconfig: Optional[QuantizationConfig] = None
    ) -> nn.Module:
        """
        é™æ€é‡åŒ– (æ¨èç”¨äºCNN)
        
        ä¼˜ç‚¹:
        - æœ€å¿«çš„æ¨ç†é€Ÿåº¦
        - æƒé‡å’Œæ¿€æ´»éƒ½é‡åŒ–
        - INT8è¿ç®—
        
        ç¼ºç‚¹:
        - éœ€è¦æ ¡å‡†æ•°æ®
        - å¯èƒ½æœ‰ç²¾åº¦æŸå¤±
        
        Args:
            model: PyTorchæ¨¡å‹
            calibration_data: æ ¡å‡†æ•°æ® (ä»£è¡¨æ€§æ ·æœ¬)
            qconfig: é‡åŒ–é…ç½®
        
        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        logger.info("ğŸ”§ Starting static quantization...")
        
        # 1. å‡†å¤‡æ¨¡å‹ (æ’å…¥è§‚å¯Ÿå™¨)
        model.eval()
        model.qconfig = quant.get_default_qconfig(self.qconfig_backend)
        model_prepared = quant.prepare(model, inplace=False)
        
        # 2. æ ¡å‡† (æ”¶é›†æ¿€æ´»å€¼ç»Ÿè®¡)
        logger.info(f"ğŸ“Š Calibrating with {len(calibration_data)} samples...")
        with torch.no_grad():
            for data in calibration_data:
                model_prepared(data)
        
        # 3. è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
        quantized_model = quant.convert(model_prepared, inplace=False)
        
        # è®¡ç®—å‹ç¼©ç‡
        original_size = self._get_model_size(model)
        quantized_size = self._get_model_size(quantized_model)
        reduction = 100 * (1 - quantized_size / original_size)
        
        logger.info(f"âœ… Static quantization complete:")
        logger.info(f"   - Original: {original_size:.2f} MB")
        logger.info(f"   - Quantized: {quantized_size:.2f} MB")
        logger.info(f"   - Reduction: {reduction:.1f}%")
        
        return quantized_model
    
    # ==================== ç»“æ„åŒ–å‰ªæ ====================
    
    def prune_structured(
        self,
        model: nn.Module,
        amount: float = 0.3,
        config: Optional[PruningConfig] = None
    ) -> nn.Module:
        """
        ç»“æ„åŒ–å‰ªæ (ç§»é™¤æ•´ä¸ªç¥ç»å…ƒ/é€šé“)
        
        ä¼˜ç‚¹:
        - å®é™…åŠ é€Ÿ (å‡å°‘è®¡ç®—)
        - ä¿æŒæ¨¡å‹ç»“æ„
        - å¯ä¸é‡åŒ–ç»“åˆ
        
        æ–¹æ³•:
        - L1èŒƒæ•°: ç§»é™¤L1èŒƒæ•°æœ€å°çš„é€šé“
        - Random: éšæœºç§»é™¤
        
        æ³¨æ„: ç»“æ„åŒ–å‰ªæä¼šè®¾ç½®æƒé‡ä¸º0ä½†ä¸å‡å°‘å‚æ•°æ•°é‡
              éœ€è¦é…åˆæ¨¡å‹é‡æ„æ‰èƒ½çœŸæ­£å‡å°‘å‚æ•°
        
        Args:
            model: PyTorchæ¨¡å‹
            amount: å‰ªææ¯”ä¾‹ (0-1)
            config: å‰ªæé…ç½®
        
        Returns:
            å‰ªæåçš„æ¨¡å‹
        """
        logger.info(f"ğŸ”§ Starting structured pruning (amount={amount})...")
        
        config = config or PruningConfig(amount=amount)
        pruned_model = copy.deepcopy(model)
        
        # å¯¹æ¯ä¸ªLinearå±‚è¿›è¡Œå‰ªæ
        total_pruned = 0
        total_params = 0
        
        for name, module in pruned_model.named_modules():
            if isinstance(module, nn.Linear):
                # è·å–åŸå§‹å‚æ•°æ•°é‡
                original_params = module.weight.numel()
                total_params += original_params
                
                # åº”ç”¨L1éç»“æ„åŒ–å‰ªæ (æ›´ç®€å•,æ›´æœ‰æ•ˆ)
                prune.l1_unstructured(module, name="weight", amount=config.amount)
                
                # è®¡ç®—å®é™…å‰ªæçš„å‚æ•°æ•°é‡
                mask = getattr(module, "weight_mask", None)
                if mask is not None:
                    pruned_count = (mask == 0).sum().item()
                    total_pruned += pruned_count
                
                # æ°¸ä¹…ç§»é™¤å‰ªææ©ç  (å°†0æƒé‡å›ºåŒ–)
                prune.remove(module, "weight")
        
        # è®¡ç®—å‰ªææ•ˆæœ (åŸºäºè®¾ç½®ä¸º0çš„æƒé‡æ•°é‡)
        if total_params > 0:
            reduction = 100 * total_pruned / total_params
        else:
            reduction = 0.0
        
        logger.info(f"âœ… Structured pruning complete:")
        logger.info(f"   - Total params: {total_params:,}")
        logger.info(f"   - Params set to zero: {total_pruned:,}")
        logger.info(f"   - Effective reduction: {reduction:.1f}%")
        logger.info(f"   - Note: Model size unchanged (need retraining for actual reduction)")
        
        return pruned_model
    
    # ==================== çŸ¥è¯†è’¸é¦ ====================
    
    def distill_knowledge(
        self,
        teacher_model: nn.Module,
        student_model: nn.Module,
        train_loader: Any,  # DataLoader
        config: Optional[DistillationConfig] = None,
        criterion: Optional[nn.Module] = None
    ) -> nn.Module:
        """
        çŸ¥è¯†è’¸é¦ (ç”¨å°æ¨¡å‹å­¦ä¹ å¤§æ¨¡å‹)
        
        åŸç†:
        - è½¯æ ‡ç­¾ (Soft Labels): teacherçš„æ¦‚ç‡åˆ†å¸ƒ
        - ç¡¬æ ‡ç­¾ (Hard Labels): çœŸå®æ ‡ç­¾
        - è’¸é¦æŸå¤± = Î± * KL(student, teacher) + (1-Î±) * CE(student, labels)
        
        Args:
            teacher_model: å¤§æ¨¡å‹ (æ•™å¸ˆ)
            student_model: å°æ¨¡å‹ (å­¦ç”Ÿ)
            train_loader: è®­ç»ƒæ•°æ®
            config: è’¸é¦é…ç½®
            criterion: æŸå¤±å‡½æ•° (é»˜è®¤CrossEntropyLoss)
        
        Returns:
            è®­ç»ƒåçš„å­¦ç”Ÿæ¨¡å‹
        """
        logger.info("ğŸ”§ Starting knowledge distillation...")
        
        config = config or DistillationConfig()
        criterion = criterion or nn.CrossEntropyLoss()
        
        # è®¾ç½®æ¨¡å¼
        teacher_model.eval()
        student_model.train()
        
        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(
            student_model.parameters(),
            lr=config.learning_rate
        )
        
        # è®­ç»ƒå¾ªç¯
        for epoch in range(config.epochs):
            total_loss = 0.0
            batch_count = 0
            
            for batch_idx, (data, target) in enumerate(train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                # æ•™å¸ˆé¢„æµ‹ (ä¸éœ€è¦æ¢¯åº¦)
                with torch.no_grad():
                    teacher_logits = teacher_model(data)
                
                # å­¦ç”Ÿé¢„æµ‹
                student_logits = student_model(data)
                
                # è½¯æ ‡ç­¾æŸå¤± (KLæ•£åº¦)
                soft_loss = self._distillation_loss(
                    student_logits,
                    teacher_logits,
                    temperature=config.temperature
                )
                
                # ç¡¬æ ‡ç­¾æŸå¤± (çœŸå®æ ‡ç­¾)
                hard_loss = criterion(student_logits, target)
                
                # æ€»æŸå¤±
                loss = config.alpha * soft_loss + (1 - config.alpha) * hard_loss
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
                batch_count += 1
            
            avg_loss = total_loss / batch_count
            logger.info(f"   Epoch {epoch+1}/{config.epochs}: Loss = {avg_loss:.4f}")
        
        logger.info("âœ… Knowledge distillation complete")
        
        return student_model
    
    # ==================== ç»„åˆä¼˜åŒ– ====================
    
    def optimize_full_pipeline(
        self,
        model: nn.Module,
        calibration_data: Optional[List[torch.Tensor]] = None,
        prune_amount: float = 0.3,
        use_quantization: bool = True,
        use_pruning: bool = True
    ) -> Tuple[nn.Module, OptimizationResult]:
        """
        å®Œæ•´ä¼˜åŒ–æµç¨‹ (å‰ªæ + é‡åŒ–)
        
        æ¨èé¡ºåº:
        1. ç»“æ„åŒ–å‰ªæ (å‡å°‘å‚æ•°)
        2. å¾®è°ƒ (æ¢å¤ç²¾åº¦)
        3. é‡åŒ– (å‹ç¼©æƒé‡)
        
        Args:
            model: åŸå§‹æ¨¡å‹
            calibration_data: æ ¡å‡†æ•°æ® (é™æ€é‡åŒ–ç”¨)
            prune_amount: å‰ªææ¯”ä¾‹
            use_quantization: æ˜¯å¦ä½¿ç”¨é‡åŒ–
            use_pruning: æ˜¯å¦ä½¿ç”¨å‰ªæ
        
        Returns:
            (ä¼˜åŒ–åçš„æ¨¡å‹, ä¼˜åŒ–ç»“æœ)
        """
        logger.info("ğŸš€ Starting full optimization pipeline...")
        
        original_size = self._get_model_size(model)
        original_latency = self._benchmark_latency(model)
        
        optimized_model = model
        
        # æ­¥éª¤1: ç»“æ„åŒ–å‰ªæ
        if use_pruning:
            logger.info("ğŸ“ Step 1/2: Structured Pruning")
            optimized_model = self.prune_structured(optimized_model, amount=prune_amount)
        
        # æ­¥éª¤2: åŠ¨æ€é‡åŒ–
        if use_quantization:
            logger.info("ğŸ“ Step 2/2: Dynamic Quantization")
            if calibration_data:
                optimized_model = self.quantize_static(optimized_model, calibration_data)
            else:
                optimized_model = self.quantize_dynamic(optimized_model)
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        optimized_size = self._get_model_size(optimized_model)
        optimized_latency = self._benchmark_latency(optimized_model)
        
        result = OptimizationResult(
            original_size_mb=original_size,
            optimized_size_mb=optimized_size,
            size_reduction_percent=100 * (1 - optimized_size / original_size),
            original_latency_ms=original_latency,
            optimized_latency_ms=optimized_latency,
            latency_reduction_percent=100 * (1 - optimized_latency / original_latency),
            accuracy_drop_percent=0.0,  # éœ€è¦é¢å¤–æµ‹è¯•
            method="prune+quantize" if use_pruning and use_quantization else "quantize"
        )
        
        logger.info("âœ… Full optimization complete:")
        logger.info(f"   - Size: {original_size:.2f} â†’ {optimized_size:.2f} MB ({result.size_reduction_percent:.1f}%)")
        logger.info(f"   - Latency: {original_latency:.1f} â†’ {optimized_latency:.1f} ms ({result.latency_reduction_percent:.1f}%)")
        
        return optimized_model, result
    
    # ==================== å·¥å…·æ–¹æ³• ====================
    
    def _get_model_size(self, model: nn.Module) -> float:
        """è®¡ç®—æ¨¡å‹å¤§å° (MB)"""
        param_size = 0
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()
        
        buffer_size = 0
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()
        
        size_mb = (param_size + buffer_size) / 1024**2
        return size_mb
    
    def _benchmark_latency(
        self,
        model: nn.Module,
        input_shape: Tuple[int, ...] = (1, 512),
        num_runs: int = 100
    ) -> float:
        """åŸºå‡†æµ‹è¯•å»¶è¿Ÿ (ms)"""
        model.eval()
        
        # åˆ›å»ºå‡è¾“å…¥
        dummy_input = torch.randn(input_shape).to(self.device)
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(10):
                _ = model(dummy_input)
        
        # æµ‹é‡
        times = []
        with torch.no_grad():
            for _ in range(num_runs):
                start = time.time()
                _ = model(dummy_input)
                end = time.time()
                times.append((end - start) * 1000)  # ms
        
        return np.median(times)
    
    def _distillation_loss(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        temperature: float = 3.0
    ) -> torch.Tensor:
        """
        è’¸é¦æŸå¤± (KLæ•£åº¦)
        
        å…¬å¼: KL(P_teacher || P_student) å…¶ä¸­ Pæ˜¯æ¸©åº¦è½¯åŒ–çš„æ¦‚ç‡åˆ†å¸ƒ
        """
        # æ¸©åº¦è½¯åŒ–
        soft_student = torch.nn.functional.log_softmax(student_logits / temperature, dim=1)
        soft_teacher = torch.nn.functional.softmax(teacher_logits / temperature, dim=1)
        
        # KLæ•£åº¦
        loss = torch.nn.functional.kl_div(
            soft_student,
            soft_teacher,
            reduction='batchmean'
        ) * (temperature ** 2)
        
        return loss
    
    def save_optimized_model(
        self,
        model: nn.Module,
        name: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Path:
        """ä¿å­˜ä¼˜åŒ–åçš„æ¨¡å‹"""
        save_path = self.cache_dir / f"{name}_optimized.pth"
        
        torch.save({
            'model_state_dict': model.state_dict(),
            'metadata': metadata or {},
            'timestamp': time.time()
        }, save_path)
        
        logger.info(f"ğŸ’¾ Model saved to {save_path}")
        return save_path
    
    def load_optimized_model(
        self,
        model: nn.Module,
        name: str
    ) -> nn.Module:
        """åŠ è½½ä¼˜åŒ–åçš„æ¨¡å‹"""
        load_path = self.cache_dir / f"{name}_optimized.pth"
        
        if not load_path.exists():
            raise FileNotFoundError(f"Model not found: {load_path}")
        
        checkpoint = torch.load(load_path)
        model.load_state_dict(checkpoint['model_state_dict'])
        
        logger.info(f"ğŸ“‚ Model loaded from {load_path}")
        return model


# ==================== ä¾¿æ·å‡½æ•° ====================

def quick_quantize(model: nn.Module) -> nn.Module:
    """å¿«é€Ÿé‡åŒ– (åŠ¨æ€, æ— éœ€æ•°æ®)"""
    optimizer = ProductionModelOptimizer()
    return optimizer.quantize_dynamic(model)


def quick_optimize(
    model: nn.Module,
    prune_amount: float = 0.3
) -> Tuple[nn.Module, OptimizationResult]:
    """å¿«é€Ÿä¼˜åŒ– (å‰ªæ + é‡åŒ–)"""
    optimizer = ProductionModelOptimizer()
    return optimizer.optimize_full_pipeline(
        model,
        prune_amount=prune_amount,
        use_quantization=True,
        use_pruning=True
    )

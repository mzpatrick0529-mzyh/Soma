"""
æµ‹è¯•æ–‡ä»¶: çœŸå®æ¨¡å‹é‡åŒ– - Phase 7B.1
éªŒè¯é‡åŒ–ã€å‰ªæã€è’¸é¦åŠŸèƒ½

æµ‹è¯•åœºæ™¯:
1. åŠ¨æ€é‡åŒ– (Linearå±‚)
2. é™æ€é‡åŒ– (CNN)
3. ç»“æ„åŒ–å‰ªæ
4. çŸ¥è¯†è’¸é¦
5. å®Œæ•´ä¼˜åŒ–æµç¨‹
"""

import torch
import torch.nn as nn
import pytest
from typing import List
import logging

from model_quantization import (
    ProductionModelOptimizer,
    QuantizationConfig,
    PruningConfig,
    DistillationConfig,
    quick_quantize,
    quick_optimize
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ==================== æµ‹è¯•æ¨¡å‹ ====================

class SimpleLinearModel(nn.Module):
    """ç®€å•çº¿æ€§æ¨¡å‹ (ç”¨äºæµ‹è¯•åŠ¨æ€é‡åŒ–)"""
    def __init__(self, input_dim: int = 512, hidden_dim: int = 256, output_dim: int = 10):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x


class SimpleCNNModel(nn.Module):
    """ç®€å•CNNæ¨¡å‹ (ç”¨äºæµ‹è¯•é™æ€é‡åŒ–)"""
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)
        self.relu = nn.ReLU()
        self.pool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(32, 10)
    
    def forward(self, x):
        x = self.conv1(x)
        x = self.relu(x)
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x


# ==================== æµ‹è¯•ç”¨ä¾‹ ====================

class TestModelQuantization:
    """æ¨¡å‹é‡åŒ–æµ‹è¯•å¥—ä»¶"""
    
    @pytest.fixture
    def optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨å®ä¾‹"""
        return ProductionModelOptimizer(device="cpu")
    
    @pytest.fixture
    def linear_model(self):
        """åˆ›å»ºçº¿æ€§æ¨¡å‹"""
        model = SimpleLinearModel()
        model.eval()
        return model
    
    @pytest.fixture
    def cnn_model(self):
        """åˆ›å»ºCNNæ¨¡å‹"""
        model = SimpleCNNModel()
        model.eval()
        return model
    
    # ==================== Test 1: åŠ¨æ€é‡åŒ– ====================
    
    def test_dynamic_quantization(self, optimizer, linear_model):
        """
        æµ‹è¯•1: åŠ¨æ€é‡åŒ–
        
        éªŒè¯:
        - æ¨¡å‹å¤§å°å‡å°‘ > 50%
        - æ¨¡å‹å¯æ­£å¸¸æ¨ç†
        - è¾“å‡ºå½¢çŠ¶ä¸å˜
        """
        logger.info("\n" + "="*60)
        logger.info("Test 1: Dynamic Quantization")
        logger.info("="*60)
        
        # åŸå§‹æ¨¡å‹
        original_size = optimizer._get_model_size(linear_model)
        logger.info(f"Original model size: {original_size:.2f} MB")
        
        # é‡åŒ–
        quantized_model = optimizer.quantize_dynamic(linear_model)
        quantized_size = optimizer._get_model_size(quantized_model)
        reduction = 100 * (1 - quantized_size / original_size)
        
        logger.info(f"Quantized model size: {quantized_size:.2f} MB")
        logger.info(f"Size reduction: {reduction:.1f}%")
        
        # éªŒè¯æ¨ç†
        dummy_input = torch.randn(1, 512)
        with torch.no_grad():
            original_output = linear_model(dummy_input)
            quantized_output = quantized_model(dummy_input)
        
        # æ–­è¨€
        assert reduction > 50, f"Size reduction {reduction:.1f}% < 50%"
        assert original_output.shape == quantized_output.shape, "Output shape mismatch"
        
        # éªŒè¯ç²¾åº¦æŸå¤± < 5%
        mse = torch.nn.functional.mse_loss(original_output, quantized_output).item()
        logger.info(f"MSE between original and quantized: {mse:.6f}")
        assert mse < 1.0, f"MSE {mse:.6f} too high"
        
        logger.info("âœ… Test 1 PASSED: Dynamic quantization works correctly")
    
    # ==================== Test 2: é™æ€é‡åŒ– ====================
    
    def test_static_quantization(self, optimizer, cnn_model):
        """
        æµ‹è¯•2: é™æ€é‡åŒ–
        
        éªŒè¯:
        - éœ€è¦æ ¡å‡†æ•°æ®
        - æ¨¡å‹å¤§å°å‡å°‘ > 60%
        - æ¨ç†é€Ÿåº¦æå‡
        """
        logger.info("\n" + "="*60)
        logger.info("Test 2: Static Quantization")
        logger.info("="*60)
        
        # åˆ›å»ºæ ¡å‡†æ•°æ®
        calibration_data = [torch.randn(1, 3, 32, 32) for _ in range(10)]
        
        # åŸå§‹æ¨¡å‹
        original_size = optimizer._get_model_size(cnn_model)
        original_latency = optimizer._benchmark_latency(
            cnn_model,
            input_shape=(1, 3, 32, 32),
            num_runs=50
        )
        
        logger.info(f"Original: {original_size:.2f} MB, {original_latency:.2f} ms")
        
        # é™æ€é‡åŒ–
        quantized_model = optimizer.quantize_static(cnn_model, calibration_data)
        quantized_size = optimizer._get_model_size(quantized_model)
        quantized_latency = optimizer._benchmark_latency(
            quantized_model,
            input_shape=(1, 3, 32, 32),
            num_runs=50
        )
        
        size_reduction = 100 * (1 - quantized_size / original_size)
        latency_reduction = 100 * (1 - quantized_latency / original_latency)
        
        logger.info(f"Quantized: {quantized_size:.2f} MB, {quantized_latency:.2f} ms")
        logger.info(f"Size reduction: {size_reduction:.1f}%")
        logger.info(f"Latency reduction: {latency_reduction:.1f}%")
        
        # æ–­è¨€
        assert size_reduction > 50, f"Size reduction {size_reduction:.1f}% < 50%"
        
        logger.info("âœ… Test 2 PASSED: Static quantization works correctly")
    
    # ==================== Test 3: ç»“æ„åŒ–å‰ªæ ====================
    
    def test_structured_pruning(self, optimizer, linear_model):
        """
        æµ‹è¯•3: ç»“æ„åŒ–å‰ªæ
        
        éªŒè¯:
        - å‚æ•°æ•°é‡å‡å°‘
        - æ¨¡å‹å¯æ­£å¸¸æ¨ç†
        """
        logger.info("\n" + "="*60)
        logger.info("Test 3: Structured Pruning")
        logger.info("="*60)
        
        # åŸå§‹å‚æ•°æ•°é‡
        original_params = sum(p.numel() for p in linear_model.parameters())
        logger.info(f"Original parameters: {original_params:,}")
        
        # å‰ªæ 30%
        pruned_model = optimizer.prune_structured(linear_model, amount=0.3)
        pruned_params = sum(p.numel() for p in pruned_model.parameters())
        reduction = 100 * (1 - pruned_params / original_params)
        
        logger.info(f"Pruned parameters: {pruned_params:,}")
        logger.info(f"Parameter reduction: {reduction:.1f}%")
        
        # éªŒè¯æ¨ç†
        dummy_input = torch.randn(1, 512)
        with torch.no_grad():
            output = pruned_model(dummy_input)
        
        # æ–­è¨€
        assert output.shape == (1, 10), "Output shape incorrect"
        assert reduction > 0, "No parameters were pruned"
        
        logger.info("âœ… Test 3 PASSED: Structured pruning works correctly")
    
    # ==================== Test 4: çŸ¥è¯†è’¸é¦ ====================
    
    def test_knowledge_distillation(self, optimizer):
        """
        æµ‹è¯•4: çŸ¥è¯†è’¸é¦
        
        éªŒè¯:
        - å­¦ç”Ÿæ¨¡å‹å¯è®­ç»ƒ
        - æŸå¤±ä¸‹é™
        """
        logger.info("\n" + "="*60)
        logger.info("Test 4: Knowledge Distillation")
        logger.info("="*60)
        
        # æ•™å¸ˆæ¨¡å‹ (å¤§)
        teacher_model = SimpleLinearModel(512, 512, 10)
        teacher_model.eval()
        
        # å­¦ç”Ÿæ¨¡å‹ (å°)
        student_model = SimpleLinearModel(512, 128, 10)
        
        # åˆ›å»ºå‡æ•°æ®
        train_data = [
            (torch.randn(4, 512), torch.randint(0, 10, (4,)))
            for _ in range(10)
        ]
        
        # è’¸é¦
        config = DistillationConfig(epochs=2, learning_rate=1e-3)
        trained_student = optimizer.distill_knowledge(
            teacher_model,
            student_model,
            train_data,
            config
        )
        
        # éªŒè¯å­¦ç”Ÿæ¨¡å‹å¤§å°æ›´å°
        teacher_size = optimizer._get_model_size(teacher_model)
        student_size = optimizer._get_model_size(trained_student)
        
        logger.info(f"Teacher size: {teacher_size:.2f} MB")
        logger.info(f"Student size: {student_size:.2f} MB")
        logger.info(f"Size reduction: {100 * (1 - student_size / teacher_size):.1f}%")
        
        # æ–­è¨€
        assert student_size < teacher_size, "Student should be smaller than teacher"
        
        logger.info("âœ… Test 4 PASSED: Knowledge distillation works correctly")
    
    # ==================== Test 5: å®Œæ•´ä¼˜åŒ–æµç¨‹ ====================
    
    def test_full_optimization_pipeline(self, optimizer, linear_model):
        """
        æµ‹è¯•5: å®Œæ•´ä¼˜åŒ–æµç¨‹
        
        éªŒè¯:
        - å‰ªæ + é‡åŒ–ç»„åˆ
        - æ€§èƒ½æå‡æ˜æ˜¾
        """
        logger.info("\n" + "="*60)
        logger.info("Test 5: Full Optimization Pipeline")
        logger.info("="*60)
        
        # å®Œæ•´ä¼˜åŒ–
        optimized_model, result = optimizer.optimize_full_pipeline(
            linear_model,
            prune_amount=0.3,
            use_quantization=True,
            use_pruning=True
        )
        
        logger.info(f"ğŸ“Š Optimization Results:")
        logger.info(f"   Size: {result.original_size_mb:.2f} â†’ {result.optimized_size_mb:.2f} MB ({result.size_reduction_percent:.1f}%)")
        logger.info(f"   Latency: {result.original_latency_ms:.1f} â†’ {result.optimized_latency_ms:.1f} ms ({result.latency_reduction_percent:.1f}%)")
        logger.info(f"   Method: {result.method}")
        
        # æ–­è¨€
        assert result.size_reduction_percent > 50, "Size reduction insufficient"
        
        logger.info("âœ… Test 5 PASSED: Full optimization pipeline works correctly")
    
    # ==================== Test 6: ä¾¿æ·å‡½æ•° ====================
    
    def test_quick_quantize(self, linear_model):
        """æµ‹è¯•6: å¿«é€Ÿé‡åŒ–å‡½æ•°"""
        logger.info("\n" + "="*60)
        logger.info("Test 6: Quick Quantize Function")
        logger.info("="*60)
        
        quantized_model = quick_quantize(linear_model)
        
        # éªŒè¯æ¨ç†
        dummy_input = torch.randn(1, 512)
        with torch.no_grad():
            output = quantized_model(dummy_input)
        
        assert output.shape == (1, 10), "Output shape incorrect"
        logger.info("âœ… Test 6 PASSED: Quick quantize works")
    
    def test_quick_optimize(self, linear_model):
        """æµ‹è¯•7: å¿«é€Ÿä¼˜åŒ–å‡½æ•°"""
        logger.info("\n" + "="*60)
        logger.info("Test 7: Quick Optimize Function")
        logger.info("="*60)
        
        optimized_model, result = quick_optimize(linear_model, prune_amount=0.2)
        
        logger.info(f"Size reduction: {result.size_reduction_percent:.1f}%")
        logger.info(f"Latency reduction: {result.latency_reduction_percent:.1f}%")
        
        assert result.size_reduction_percent > 40, "Optimization insufficient"
        logger.info("âœ… Test 7 PASSED: Quick optimize works")


# ==================== é›†æˆæµ‹è¯• ====================

def test_integration_real_world_scenario():
    """
    é›†æˆæµ‹è¯•: çœŸå®åœºæ™¯æ¨¡æ‹Ÿ
    
    åœºæ™¯: éƒ¨ç½²ä¸€ä¸ªå¤§å‹çº¿æ€§æ¨¡å‹åˆ°ç§»åŠ¨è®¾å¤‡
    ç›®æ ‡: æ¨¡å‹å¤§å° < 100MB, å»¶è¿Ÿ < 100ms
    """
    logger.info("\n" + "="*60)
    logger.info("Integration Test: Real-World Scenario")
    logger.info("="*60)
    
    # åˆ›å»ºå¤§æ¨¡å‹
    large_model = SimpleLinearModel(input_dim=1024, hidden_dim=2048, output_dim=100)
    large_model.eval()
    
    optimizer = ProductionModelOptimizer()
    
    # åŸå§‹æŒ‡æ ‡
    original_size = optimizer._get_model_size(large_model)
    original_latency = optimizer._benchmark_latency(
        large_model,
        input_shape=(1, 1024),
        num_runs=50
    )
    
    logger.info(f"ğŸ¯ Target: Size < 100MB, Latency < 100ms")
    logger.info(f"ğŸ“Š Original: Size = {original_size:.2f} MB, Latency = {original_latency:.2f} ms")
    
    # å®Œæ•´ä¼˜åŒ–
    optimized_model, result = optimizer.optimize_full_pipeline(
        large_model,
        prune_amount=0.4,  # æ›´æ¿€è¿›çš„å‰ªæ
        use_quantization=True,
        use_pruning=True
    )
    
    logger.info(f"âœ… Optimized: Size = {result.optimized_size_mb:.2f} MB, Latency = {result.optimized_latency_ms:.2f} ms")
    
    # éªŒè¯ç›®æ ‡è¾¾æˆ
    if result.optimized_size_mb < 100:
        logger.info("âœ… Size target achieved!")
    else:
        logger.warning(f"âš ï¸ Size target missed: {result.optimized_size_mb:.2f} MB > 100 MB")
    
    if result.optimized_latency_ms < 100:
        logger.info("âœ… Latency target achieved!")
    else:
        logger.warning(f"âš ï¸ Latency target missed: {result.optimized_latency_ms:.2f} ms > 100 ms")
    
    logger.info("ğŸ‰ Integration test complete!")


# ==================== ä¸»å‡½æ•° ====================

if __name__ == "__main__":
    logger.info("="*60)
    logger.info("Production Model Quantization Tests - Phase 7B.1")
    logger.info("="*60)
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    pytest.main([__file__, "-v", "-s"])
    
    # è¿è¡Œé›†æˆæµ‹è¯•
    test_integration_real_world_scenario()

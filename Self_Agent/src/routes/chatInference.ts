/**
 * Self Agent Chat Inference Routes
 * äººæ ¼æ¨¡åž‹æŽ¨ç† API - ç”¨äºŽç”Ÿæˆä¸ªæ€§åŒ–å›žå¤
 */

import express, { Request, Response } from 'express';
// Queue-based execution
import { enqueueInference, enqueueRerank } from '../queue/queues.js';
import { getDB, getUserAvailableSources } from '../db/index.js';
import { retrieveRelevantHybrid } from '../pipeline/rag.js';
import { buildPersonaProfile, buildPersonaPrompt } from '../pipeline/persona.js';
import { detectSourceIntent, rerankCandidates, composeCitedContext, compressHistory, calibrateStyle } from '../pipeline/rag_enhance.js';
import { rerankWithCrossEncoder } from '../services/reranker.js';
import path from 'path';

export const chatInferenceRouter = express.Router();

/**
 * POST /api/self-agent/chat/inference
 * ä½¿ç”¨è®­ç»ƒå¥½çš„äººæ ¼æ¨¡åž‹ç”Ÿæˆå›žå¤
 */
chatInferenceRouter.post('/inference', async (req: Request, res: Response) => {
  try {
    const {
      userId,
      message,
      conversationHistory = [],
      useRAG = true,
      temperature = 0.8,
      maxTokens = 150,
      rag = {},
      convo = {},
    } = req.body;

    if (!userId || !message) {
      return res.status(400).json({
        error: 'userId and message are required'
      });
    }

    const db = getDB();

    // 1. æ£€æŸ¥ç”¨æˆ·æ˜¯å¦æœ‰è®­ç»ƒå¥½çš„æ¨¡åž‹
    const model = (db.prepare(`
      SELECT model_path, model_version, created_at
      FROM personality_models
      WHERE user_id = ? AND is_active = 1
      ORDER BY created_at DESC
      LIMIT 1
    `).get(userId) as any);

    if (!model) {
      return res.status(404).json({
        error: 'No trained model found',
        message: 'ç”¨æˆ·è¿˜æ²¡æœ‰è®­ç»ƒäººæ ¼æ¨¡åž‹ï¼Œè¯·å…ˆå¯¼å…¥æ•°æ®å¹¶è®­ç»ƒ',
        suggestion: 'POST /api/self-agent/training/trigger'
      });
    }

    console.log(`\nðŸ¤– Generating response for user: ${userId}`);
    console.log(`   Model: ${model.model_path}`);
    console.log(`   Message: ${message.substring(0, 50)}...`);

    // 2. RAG å¢žå¼ºï¼šæ¥æºæ„å›¾ + æ··åˆæ£€ç´¢ + é‡æŽ’åº + å¼•ç”¨ä¸Šä¸‹æ–‡æ‹¼æŽ¥
    let relevantMemories: string[] = [];
    let citedContext = '';
    let predictedSources: string[] = [];
    try {
      if (useRAG) {
        const intent = detectSourceIntent(message);
        predictedSources = intent.sources;
        const topK: number = Math.max(1, Math.min(12, rag.topK ?? 8));
        const minScore: number = typeof rag.minScore === 'number' ? rag.minScore : 0.1;
        const recentBoost: number = typeof rag.recentBoost === 'number' ? rag.recentBoost : 0.15;
        const strict: boolean = !!rag.strictSourceFilter;

        // æ··åˆæ£€ç´¢ï¼ˆè‹¥ä¸ä¸¥æ ¼è¿‡æ»¤ï¼Œåˆ™å…ˆä¸å¸¦ sourcesï¼Œç”± reranker åå¥½åŠ æƒï¼‰
        const initial = retrieveRelevantHybrid(
          userId,
          message,
          {
            topK: topK,
            minScore,
            sources: strict && predictedSources.length ? predictedSources : undefined,
            recentBoost,
          }
        );

        // é‡æŽ’åºï¼šå¯¹æ¥æºè¿›è¡Œè½»æƒé‡æå‡ï¼Œå¹¶ç»“åˆè¯é¢é‡åˆ
        let reranked = rerankCandidates(
          message,
          initial.map(r => ({ id: r.id, text: r.text, score: r.score })),
          { preferredSources: !strict ? predictedSources : [], topK: Math.min(4, topK) }
        );

        // å¯é€‰ï¼šä½¿ç”¨ Cross-Encoder è¿›ä¸€æ­¥é‡æŽ’
        try {
          // Prefer queue-based rerank if redis available; fallback to local
          if (process.env.CROSS_ENCODER && process.env.CROSS_ENCODER !== '0') {
            try {
              const rr = await enqueueRerank({ query: message, candidates: reranked.map(x => x.text), model: process.env.CROSS_ENCODER_MODEL });
              const ceScores = rr?.scores;
              if (ceScores && ceScores.length === reranked.length) {
                reranked = reranked.map((r, i) => ({ ...r, score: r.score * 0.5 + ceScores[i] * 0.5 }))
                  .sort((a, b) => b.score - a.score);
              }
            } catch {
              const ceScores = await rerankWithCrossEncoder(message, reranked.map(x => x.text), process.env.CROSS_ENCODER_MODEL);
              if (ceScores && ceScores.length === reranked.length) {
                reranked = reranked.map((r, i) => ({ ...r, score: r.score * 0.5 + ceScores[i] * 0.5 }))
                  .sort((a, b) => b.score - a.score);
              }
            }
          }
        } catch {}

        const composed = composeCitedContext(reranked, { maxSnippets: Math.min(4, topK), maxCharsPerSnippet: 260, dedup: true });
        citedContext = composed.contextText;
        relevantMemories = reranked.map(r => r.text);
        console.log(`   Retrieved ${relevantMemories.length} relevant memories (after rerank)`);
      }
    } catch (ragError) {
      console.warn('   RAG retrieval failed:', ragError);
      // ç»§ç»­æ‰§è¡Œï¼Œåªæ˜¯æ²¡æœ‰è®°å¿†å¢žå¼º
    }

    // 2.1 ä¼šè¯çª—å£åŽ‹ç¼©
    let compressedHistory = '';
    try {
      if (Array.isArray(conversationHistory) && conversationHistory.length) {
        const keepLast = Math.max(0, Math.min(10, convo.keepLast ?? 4));
        const maxChars = Math.max(200, Math.min(4000, convo.maxChars ?? 1200));
        compressedHistory = compressHistory(conversationHistory, { keepLast, maxChars });
      }
    } catch (histErr) {
      console.warn('   Conversation compression failed:', histErr);
    }

    // 2.2 äººæ ¼æç¤ºï¼ˆä¸Žå¯ç”¨æ•°æ®æºï¼‰æž„å»º
    const availableSources = getUserAvailableSources(userId);
    const persona = buildPersonaProfile(userId, { maxChunks: 120 });
    const personaPrompt = buildPersonaPrompt(persona, citedContext, availableSources);

    // å°† RAG ä¸Šä¸‹æ–‡ä¸Žä¼šè¯åŽ‹ç¼©åˆå¹¶åˆ°æœ€ç»ˆä¼ å…¥æ¨¡åž‹çš„æ¶ˆæ¯ä¸­ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼Œç›´åˆ° Python æŽ¥å£æ”¯æŒä¸Šä¸‹æ–‡å‚æ•°ï¼‰
    const finalMessage = [
      personaPrompt ? `ã€äººæ ¼ä¸Žè®°å¿†ä¸Šä¸‹æ–‡ã€‘\n${personaPrompt}` : '',
      compressedHistory ? `ã€å¯¹è¯åŽ†å²ã€‘\n${compressedHistory}` : '',
      `ã€å½“å‰é—®é¢˜ã€‘\n${message}`
    ].filter(Boolean).join('\n\n');

    // 3. è°ƒç”¨ Python æŽ¨ç†è„šæœ¬
  const pythonScript = path.join(process.cwd(), 'src/ml/personality_inference.py');
    
    // æž„å»ºè¾“å…¥æ•°æ®
    const inputData = {
      userId,
      message,
      conversationHistory,
      relevantMemories,
      config: {
        temperature,
        max_new_tokens: maxTokens
      }
    };

    // ä½¿ç”¨é˜Ÿåˆ—æ‰§è¡Œ Python æŽ¨ç†ï¼Œå¸¦è¶…æ—¶/é‡è¯•
    const stdinPayload = {
      personaPrompt: personaPrompt,
      conversationHistory,
      relevantMemories,
      message: finalMessage,
      config: { temperature, max_new_tokens: maxTokens }
    };
    try {
      const result = await enqueueInference({ userId, pythonScript, stdinPayload, timeoutMs: 45_000 });
      const calibrated = calibrateStyle(result.response, { language_style: persona.language_style });
      return res.json({
        success: true,
        response: calibrated,
        metadata: {
          modelVersion: model.model_version,
          modelPath: model.model_path,
          usedMemories: relevantMemories.length,
          temperature,
          predictedSources,
          ...result.metadata
        }
      });
    } catch (e: any) {
      return res.status(500).json({ error: 'Inference failed', message: String(e?.message || e) });
    }

  } catch (error: any) {
    console.error('Inference API error:', error);
    res.status(500).json({
      error: 'Internal server error',
      message: error.message
    });
  }
});

/**
 * GET /api/self-agent/chat/model-status/:userId
 * æŸ¥è¯¢ç”¨æˆ·çš„æ¨¡åž‹çŠ¶æ€
 */
chatInferenceRouter.get('/model-status/:userId', (req: Request, res: Response) => {
  try {
    const { userId } = req.params;

    if (!userId) {
      return res.status(400).json({ error: 'userId is required' });
    }

    const db = getDB();

    // æŸ¥è¯¢æœ€æ–°æ¨¡åž‹
    const model = (db.prepare(`
      SELECT 
        model_version,
        model_type,
        model_path,
        training_samples_count,
        training_loss,
        created_at,
        is_active
      FROM personality_models
      WHERE user_id = ?
      ORDER BY created_at DESC
      LIMIT 1
    `).get(userId) as any);

    if (!model) {
      return res.json({
        hasModel: false,
        message: 'ç”¨æˆ·è¿˜æ²¡æœ‰è®­ç»ƒäººæ ¼æ¨¡åž‹'
      });
    }

    // æŸ¥è¯¢è®­ç»ƒæ ·æœ¬ç»Ÿè®¡
    const sampleStats = (db.prepare(`
      SELECT 
        COUNT(*) as total,
        SUM(CASE WHEN used_for_training = 1 THEN 1 ELSE 0 END) as used,
        AVG(quality_score) as avg_quality
      FROM personality_training_samples
      WHERE user_id = ?
    `).get(userId) as any);

    res.json({
      hasModel: true,
      model: {
        version: model.model_version,
        type: model.model_type,
        path: model.model_path,
        trainingSamples: model.training_samples_count,
        trainingLoss: model.training_loss,
        createdAt: model.created_at,
        isActive: model.is_active === 1
      },
      samples: {
        total: sampleStats?.total || 0,
        used: sampleStats?.used || 0,
        avgQuality: sampleStats?.avg_quality || 0
      }
    });

  } catch (error: any) {
    console.error('Failed to get model status:', error);
    res.status(500).json({
      error: 'Failed to get model status',
      message: error.message
    });
  }
});

/**
 * POST /api/self-agent/chat/batch-inference
 * æ‰¹é‡ç”Ÿæˆå›žå¤ï¼ˆç”¨äºŽè¯„ä¼°ï¼‰
 */
chatInferenceRouter.post('/batch-inference', async (req: Request, res: Response) => {
  try {
    const {
      userId,
      messages = [],
      config = {}
    } = req.body;

    if (!userId || !Array.isArray(messages) || messages.length === 0) {
      return res.status(400).json({
        error: 'userId and messages array are required'
      });
    }

    if (messages.length > 20) {
      return res.status(400).json({
        error: 'Too many messages',
        message: 'Maximum 20 messages per batch'
      });
    }

    const db = getDB();

    // æ£€æŸ¥æ¨¡åž‹
    const model = (db.prepare(`
      SELECT model_path
      FROM personality_models
      WHERE user_id = ? AND is_active = 1
      LIMIT 1
    `).get(userId) as any);

    if (!model) {
      return res.status(404).json({
        error: 'No trained model found'
      });
    }

    console.log(`\nðŸ“Š Batch inference for ${messages.length} messages`);

    // ä¾æ¬¡ç”Ÿæˆå›žå¤ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå®žé™…å¯ä»¥å¹¶è¡Œï¼‰
    const results: any[] = [];

    for (const message of messages) {
      try {
        // è¿™é‡Œå¤ç”¨å•ä¸ªæŽ¨ç†é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰
        // å®žé™…ç”Ÿäº§ä¸­åº”è¯¥æ‰¹é‡å¤„ç†ä»¥æé«˜æ•ˆçŽ‡
        
  const ragResults = retrieveRelevantHybrid(userId, message, { topK: 2 });
  const relevantMemories = ragResults.map((r) => r.text);

        // TODO: å®žé™…è°ƒç”¨ Python æŽ¨ç†
        // ç›®å‰è¿”å›žå ä½ç¬¦
        results.push({
          message,
          response: `[æŽ¨ç†å›žå¤: ${message}]`,
          usedMemories: relevantMemories.length
        });

      } catch (error: any) {
        results.push({
          message,
          error: error.message
        });
      }
    }

    res.json({
      success: true,
      results,
      total: messages.length,
      succeeded: results.filter(r => !r.error).length
    });

  } catch (error: any) {
    console.error('Batch inference error:', error);
    res.status(500).json({
      error: 'Batch inference failed',
      message: error.message
    });
  }
});

export default chatInferenceRouter;

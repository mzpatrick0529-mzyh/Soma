#!/usr/bin/env python3
"""
äººæ ¼æ¨ç†å¼•æ“ - Personality Inference Engine
ç”¨äºåŠ è½½è®­ç»ƒå¥½çš„äººæ ¼æ¨¡å‹å¹¶ç”Ÿæˆä¸ªæ€§åŒ–å›å¤
"""

import os
import json
import sqlite3
from typing import List, Dict, Optional, Tuple
import sys
from dataclasses import dataclass

try:
    import torch
    from transformers import AutoModelForCausalLM, AutoTokenizer
    from peft import PeftModel
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False
    print("Warning: PyTorch not installed. Run: pip install torch transformers peft")


@dataclass
class InferenceConfig:
    """æ¨ç†é…ç½®"""
    max_new_tokens: int = 150
    temperature: float = 0.8
    top_p: float = 0.92
    top_k: int = 50
    repetition_penalty: float = 1.15
    do_sample: bool = True
    num_beams: int = 1


class PersonalityInferenceEngine:
    """äººæ ¼æ¨ç†å¼•æ“"""
    
    def __init__(
        self,
        user_id: str,
        db_path: str = './self_agent.db',
        base_model: str = "google/gemma-2b",
        model_path: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–æ¨ç†å¼•æ“
        
        Args:
            user_id: ç”¨æˆ·ID
            db_path: æ•°æ®åº“è·¯å¾„
            base_model: åŸºç¡€æ¨¡å‹åç§°
            model_path: LoRAæ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœä¸ºNoneï¼Œä»æ•°æ®åº“æŸ¥æ‰¾æœ€æ–°ç‰ˆæœ¬ï¼‰
        """
        if not HAS_TORCH:
            raise ImportError("PyTorch not installed. Cannot perform inference.")
        
        self.user_id = user_id
        self.db_path = db_path
        self.base_model = base_model
        
        # å¦‚æœæœªæŒ‡å®šæ¨¡å‹è·¯å¾„ï¼Œä»æ•°æ®åº“åŠ è½½æœ€æ–°ç‰ˆæœ¬
        if model_path is None:
            model_path = self._get_latest_model_path()
            if model_path is None:
                raise ValueError(
                    f"No trained model found for user {user_id}. "
                    f"Please train a model first."
                )
        
        self.model_path = model_path
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ”„ Loading personality model for user: {user_id}")
        print(f"   Base model: {base_model}")
        print(f"   LoRA adapter: {model_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(base_model)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½åŸºç¡€æ¨¡å‹
        self.base = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            low_cpu_mem_usage=True
        )
        
        # åŠ è½½LoRAæƒé‡
        self.model = PeftModel.from_pretrained(self.base, model_path)
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        print(f"âœ… Model loaded successfully!")
        print(f"   Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}")
        
    def _get_latest_model_path(self) -> Optional[str]:
        """ä»æ•°æ®åº“è·å–æœ€æ–°çš„æ¨¡å‹è·¯å¾„"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT model_path 
            FROM personality_models
            WHERE user_id = ? AND is_active = 1
            ORDER BY created_at DESC
            LIMIT 1
        """, (self.user_id,))
        
        result = cursor.fetchone()
        conn.close()
        
        return result[0] if result else None
    
    def generate_response(
        self,
        message: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        relevant_memories: Optional[List[str]] = None,
        persona_prompt: Optional[str] = None,
        config: Optional[InferenceConfig] = None
    ) -> Tuple[str, Dict]:
        """
        ç”Ÿæˆä¸ªæ€§åŒ–å›å¤
        
        Args:
            message: å½“å‰è¾“å…¥æ¶ˆæ¯
            conversation_history: å¯¹è¯å†å² [{"role": "user/assistant", "content": "..."}]
            relevant_memories: ç›¸å…³è®°å¿†ï¼ˆä»RAGæ£€ç´¢ï¼‰
            config: æ¨ç†é…ç½®
            
        Returns:
            (response, metadata): å›å¤å†…å®¹å’Œå…ƒæ•°æ®
        """
        if config is None:
            config = InferenceConfig()
        
        # æ„å»ºè¾“å…¥æç¤º
        prompt = self._build_prompt(message, conversation_history, relevant_memories, persona_prompt)
        
        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors='pt', truncation=True, max_length=2048)
        
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=config.max_new_tokens,
                temperature=config.temperature,
                top_p=config.top_p,
                top_k=config.top_k,
                repetition_penalty=config.repetition_penalty,
                do_sample=config.do_sample,
                num_beams=config.num_beams,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        # è§£ç 
        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–å›å¤éƒ¨åˆ†ï¼ˆå»é™¤promptï¼‰
        response = self._extract_response(full_response, prompt)
        
        # å…ƒæ•°æ®
        metadata = {
            'model_path': self.model_path,
            'prompt_length': len(inputs['input_ids'][0]),
            'response_length': len(self.tokenizer.encode(response)),
            'temperature': config.temperature,
            'used_memories': len(relevant_memories) if relevant_memories else 0
        }
        
        return response, metadata
    
    def _build_prompt(
        self,
        message: str,
        conversation_history: Optional[List[Dict[str, str]]],
        relevant_memories: Optional[List[str]],
        persona_prompt: Optional[str] = None
    ) -> str:
        """æ„å»ºæ¨ç†æç¤º"""
        parts = []

        # 0. äººæ ¼ä¸å¯ç”¨æ•°æ®æºï¼ˆè‹¥ä¼ å…¥ï¼‰
        if persona_prompt:
            parts.append("### äººæ ¼ä¸å¯ç”¨æ•°æ®æº\n" + persona_prompt.strip())
        
        # 1. ç›¸å…³è®°å¿†ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if relevant_memories and len(relevant_memories) > 0:
            parts.append("### ç›¸å…³è®°å¿†èƒŒæ™¯:")
            for i, memory in enumerate(relevant_memories[:3], 1):  # æœ€å¤š3æ¡
                parts.append(f"{i}. {memory[:200]}...")  # æˆªæ–­è¿‡é•¿è®°å¿†
            parts.append("")
        
        # 2. å¯¹è¯å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
        if conversation_history and len(conversation_history) > 0:
            parts.append("### å¯¹è¯å†å²:")
            for turn in conversation_history[-5:]:  # æœ€è¿‘5è½®å¯¹è¯
                role = "æˆ‘" if turn['role'] == 'assistant' else "å¯¹æ–¹"
                parts.append(f"{role}: {turn['content']}")
            parts.append("")
        
        # 3. å½“å‰æ¶ˆæ¯
        parts.append("### å½“å‰æ¶ˆæ¯:")
        parts.append(f"å¯¹æ–¹: {message}")
        parts.append("")
        
        # 4. ç”ŸæˆæŒ‡ä»¤
        parts.append("### å›å¤:")
        
        return "\n".join(parts)
    
    def _extract_response(self, full_text: str, prompt: str) -> str:
        """ä»ç”Ÿæˆçš„å®Œæ•´æ–‡æœ¬ä¸­æå–å›å¤éƒ¨åˆ†"""
        # å°è¯•æ‰¾åˆ°"### å›å¤:"ä¹‹åçš„å†…å®¹
        if "### å›å¤:" in full_text:
            response = full_text.split("### å›å¤:")[-1].strip()
        else:
            # å¦‚æœæ‰¾ä¸åˆ°æ ‡è®°ï¼Œå°è¯•ç§»é™¤prompt
            response = full_text[len(prompt):].strip()
        
        # æ¸…ç†å¯èƒ½çš„å¤šä½™æ ‡è®°
        response = response.replace(self.tokenizer.eos_token, "")
        response = response.strip()
        
        # å¦‚æœå›å¤å¤ªçŸ­ï¼Œå¯èƒ½æ˜¯ç”Ÿæˆå¤±è´¥
        if len(response) < 2:
            return "[æ¨¡å‹æœªèƒ½ç”Ÿæˆæœ‰æ•ˆå›å¤]"
        
        return response
    
    def batch_generate(
        self,
        messages: List[str],
        config: Optional[InferenceConfig] = None,
        batch_size: int = 4
    ) -> List[Tuple[str, Dict]]:
        """
        æ‰¹é‡ç”Ÿæˆå›å¤ï¼ˆç”¨äºè¯„ä¼°ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            config: æ¨ç†é…ç½®
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            [(response, metadata), ...]: å›å¤åˆ—è¡¨
        """
        results = []
        
        for i in range(0, len(messages), batch_size):
            batch = messages[i:i+batch_size]
            
            for msg in batch:
                response, metadata = self.generate_response(msg, config=config)
                results.append((response, metadata))
        
        return results


def test_inference():
    """æµ‹è¯•æ¨ç†å¼•æ“"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Test personality inference')
    parser.add_argument('--user-id', required=True, help='User ID')
    parser.add_argument('--message', required=True, help='Input message')
    parser.add_argument('--db-path', default='./self_agent.db', help='Database path')
    parser.add_argument('--temperature', type=float, default=0.8, help='Sampling temperature')
    parser.add_argument('--stdin-json', action='store_true', help='Read JSON context from stdin')
    
    args = parser.parse_args()

    # è¯»å– stdin ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
    persona_prompt: Optional[str] = None
    conversation_history: Optional[List[Dict[str, str]]] = None
    relevant_memories: Optional[List[str]] = None
    if args.stdin_json:
        try:
            raw = sys.stdin.read()
            if raw:
                ctx = json.loads(raw)
                persona_prompt = ctx.get('personaPrompt')
                conversation_history = ctx.get('conversationHistory')
                relevant_memories = ctx.get('relevantMemories')
                # è‹¥ stdin é‡Œä¹Ÿæä¾› messageï¼Œä¼˜å…ˆä½¿ç”¨
                if 'message' in ctx:
                    args.message = ctx['message']
        except Exception as e:
            print(f"Warning: failed to parse stdin JSON: {e}")

    # åˆ›å»ºæ¨ç†å¼•æ“
    engine = PersonalityInferenceEngine(
        user_id=args.user_id,
        db_path=args.db_path
    )
    
    # ç”Ÿæˆå›å¤
    config = InferenceConfig(temperature=args.temperature)
    response, metadata = engine.generate_response(
        args.message,
        conversation_history=conversation_history,
        relevant_memories=relevant_memories,
        persona_prompt=persona_prompt,
        config=config
    )
    
    print(f"\n{'='*60}")
    print(f"ğŸ’¬ è¾“å…¥: {args.message}")
    print(f"ğŸ¤– å›å¤: {response}")
    print(f"{'='*60}")
    print(f"\nğŸ“Š å…ƒæ•°æ®:")
    for key, value in metadata.items():
        print(f"   {key}: {value}")


if __name__ == '__main__':
    test_inference()

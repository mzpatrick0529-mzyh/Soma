"""
å¤šä»»åŠ¡è®­ç»ƒæ¡†æ¶
æ”¯æŒè”åˆä¼˜åŒ–: Generation + Style Consistency + Relationship-Aware + Contrastive Learning
"""

import os
import sys
import json
import sqlite3
import argparse
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""
    user_id: str
    db_path: str
    
    # æ¨¡å‹å‚æ•°
    base_model: str = "gemini-2.0-flash-exp"
    model_type: str = "lora"
    lora_rank: int = 16
    lora_alpha: int = 32
    
    # è®­ç»ƒè¶…å‚æ•°
    epochs: int = 3
    batch_size: int = 4
    learning_rate: float = 2e-5
    warmup_steps: int = 100
    gradient_accumulation_steps: int = 4
    
    # å¤šä»»åŠ¡æŸå¤±æƒé‡
    gen_loss_weight: float = 1.0
    style_loss_weight: float = 0.3
    relation_loss_weight: float = 0.2
    contrastive_loss_weight: float = 0.1
    
    # æ•°æ®é…ç½®
    use_augmented_data: bool = True
    min_samples: int = 50
    max_samples: Optional[int] = None
    validation_split: float = 0.1
    
    # è®­ç»ƒç­–ç•¥
    use_mixed_precision: bool = True
    use_gradient_checkpointing: bool = True
    early_stopping_patience: int = 3
    
    # è¾“å‡ºé…ç½®
    output_dir: str = "./models"
    save_steps: int = 500
    logging_steps: int = 50


@dataclass
class TrainingBatch:
    """è®­ç»ƒæ‰¹æ¬¡æ•°æ®"""
    input_ids: np.ndarray
    attention_mask: np.ndarray
    labels: np.ndarray
    
    # å¤šä»»åŠ¡è¾…åŠ©è¾“å…¥
    style_embeddings: Optional[np.ndarray] = None
    relationship_context: Optional[np.ndarray] = None
    is_negative: Optional[np.ndarray] = None  # è´Ÿæ ·æœ¬æ ‡è®°
    
    # å…ƒæ•°æ®
    sample_ids: Optional[List[str]] = None
    target_persons: Optional[List[str]] = None


class MultiTaskPersonaTrainer:
    """å¤šä»»åŠ¡äººæ ¼è®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.db_conn = sqlite3.connect(config.db_path)
        self.db_conn.row_factory = sqlite3.Row
        
        # è®­ç»ƒç»Ÿè®¡
        self.training_stats = {
            'total_steps': 0,
            'best_val_loss': float('inf'),
            'epochs_without_improvement': 0,
            'losses': {
                'gen': [],
                'style': [],
                'relation': [],
                'contrastive': [],
                'total': []
            }
        }
        
        logger.info(f"ğŸš€ Initialized MultiTaskPersonaTrainer for user: {config.user_id}")
        logger.info(f"   Base model: {config.base_model}")
        logger.info(f"   Loss weights: Gen={config.gen_loss_weight}, Style={config.style_loss_weight}, "
                   f"Relation={config.relation_loss_weight}, Contrastive={config.contrastive_loss_weight}")
    
    def train(self) -> Dict[str, Any]:
        """æ‰§è¡Œå¤šä»»åŠ¡è®­ç»ƒ"""
        logger.info("\n" + "="*80)
        logger.info("ğŸ¯ Starting Multi-Task Persona Training")
        logger.info("="*80)
        
        try:
            # 1. åŠ è½½æ•°æ®
            train_data, val_data = self._load_training_data()
            logger.info(f"âœ“ Loaded {len(train_data)} training samples, {len(val_data)} validation samples")
            
            # 2. æå–ç”¨æˆ·é£æ ¼profile
            style_profile = self._extract_style_profile()
            logger.info(f"âœ“ Extracted user style profile")
            
            # 3. æ„å»ºå…³ç³»æ˜ å°„
            relationship_map = self._build_relationship_map()
            logger.info(f"âœ“ Built relationship map with {len(relationship_map)} persons")
            
            # 4. å‡†å¤‡æ¨¡å‹ (è¿™é‡Œæ˜¯ä¼ªä»£ç ,å®é™…éœ€è¦é›†æˆçœŸå®æ¨¡å‹)
            model = self._initialize_model()
            logger.info(f"âœ“ Initialized model: {self.config.base_model}")
            
            # 5. è®­ç»ƒå¾ªç¯
            best_model_path = None
            for epoch in range(self.config.epochs):
                logger.info(f"\nğŸ“ˆ Epoch {epoch + 1}/{self.config.epochs}")
                
                # è®­ç»ƒä¸€ä¸ªepoch
                train_loss = self._train_epoch(
                    model, 
                    train_data, 
                    style_profile, 
                    relationship_map
                )
                
                # éªŒè¯
                val_loss = self._validate_epoch(
                    model, 
                    val_data, 
                    style_profile, 
                    relationship_map
                )
                
                logger.info(f"   Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")
                
                # Early stoppingæ£€æŸ¥
                if val_loss < self.training_stats['best_val_loss']:
                    self.training_stats['best_val_loss'] = val_loss
                    self.training_stats['epochs_without_improvement'] = 0
                    
                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    best_model_path = self._save_checkpoint(model, epoch, val_loss)
                    logger.info(f"   âœ“ New best model saved: {best_model_path}")
                else:
                    self.training_stats['epochs_without_improvement'] += 1
                    if self.training_stats['epochs_without_improvement'] >= self.config.early_stopping_patience:
                        logger.info(f"   âš ï¸  Early stopping triggered (patience={self.config.early_stopping_patience})")
                        break
            
            # 6. ä¿å­˜æœ€ç»ˆæ¨¡å‹ä¸å…ƒæ•°æ®
            final_metadata = self._save_final_model(best_model_path)
            
            logger.info("\n" + "="*80)
            logger.info("âœ… Training Completed Successfully!")
            logger.info(f"   Best Val Loss: {self.training_stats['best_val_loss']:.4f}")
            logger.info(f"   Model saved to: {final_metadata['model_path']}")
            logger.info("="*80 + "\n")
            
            return final_metadata
            
        except Exception as e:
            logger.error(f"âŒ Training failed: {str(e)}")
            raise
        finally:
            self.db_conn.close()
    
    def _load_training_data(self) -> Tuple[List[Dict], List[Dict]]:
        """åŠ è½½è®­ç»ƒæ•°æ®"""
        cursor = self.db_conn.cursor()
        
        # é€‰æ‹©æ•°æ®æº
        if self.config.use_augmented_data:
            # ä½¿ç”¨å¢å¼ºæ•°æ®
            table = 'enhanced_training_samples'
            logger.info("   Using augmented training samples")
        else:
            # ä½¿ç”¨åŸå§‹æ•°æ®
            table = 'personality_training_samples'
            logger.info("   Using original training samples")
        
        # æŸ¥è¯¢æ•°æ®
        query = f"""
            SELECT 
                id, conversation_context, user_response, target_person,
                emotional_context, quality_score, style_tags, intent_tags,
                augmentation_type, augmentation_metadata
            FROM {table}
            WHERE user_id = ? AND used_for_training = 0
            ORDER BY quality_score DESC
        """
        
        if self.config.max_samples:
            query += f" LIMIT {self.config.max_samples}"
        
        cursor.execute(query, (self.config.user_id,))
        
        all_samples = []
        for row in cursor.fetchall():
            sample = dict(row)
            
            # è§£æJSONå­—æ®µ
            if 'augmentation_metadata' in sample and sample['augmentation_metadata']:
                try:
                    sample['augmentation_metadata'] = json.loads(sample['augmentation_metadata'])
                except:
                    sample['augmentation_metadata'] = {}
            
            all_samples.append(sample)
        
        if len(all_samples) < self.config.min_samples:
            raise ValueError(
                f"Insufficient samples: {len(all_samples)} < {self.config.min_samples}. "
                "Consider running sample augmentation first."
            )
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        np.random.shuffle(all_samples)
        split_idx = int(len(all_samples) * (1 - self.config.validation_split))
        
        train_data = all_samples[:split_idx]
        val_data = all_samples[split_idx:]
        
        return train_data, val_data
    
    def _extract_style_profile(self) -> Dict[str, Any]:
        """æå–ç”¨æˆ·é£æ ¼profile"""
        cursor = self.db_conn.cursor()
        
        # ä»æ‰€æœ‰ç”¨æˆ·å›å¤ä¸­åˆ†æé£æ ¼
        cursor.execute("""
            SELECT user_response, style_tags
            FROM personality_training_samples
            WHERE user_id = ?
        """, (self.config.user_id,))
        
        responses = [row[0] for row in cursor.fetchall()]
        
        if not responses:
            logger.warning("   No responses found for style extraction, using defaults")
            return self._get_default_style_profile()
        
        # é£æ ¼ç‰¹å¾æå–
        profile = {
            'avg_length': np.mean([len(r) for r in responses]),
            'exclamation_freq': sum(r.count('!') for r in responses) / len(responses),
            'question_freq': sum(r.count('?') for r in responses) / len(responses),
            'emoji_freq': sum(len([c for c in r if ord(c) > 0x1F300]) for r in responses) / len(responses),
            
            # ç®€åŒ–çš„embedding (å®é™…åº”ç”¨çœŸå®embeddingæ¨¡å‹)
            'style_embedding': np.random.randn(128).tolist()  # å ä½ç¬¦
        }
        
        return profile
    
    def _get_default_style_profile(self) -> Dict[str, Any]:
        """é»˜è®¤é£æ ¼profile"""
        return {
            'avg_length': 30,
            'exclamation_freq': 0.1,
            'question_freq': 0.05,
            'emoji_freq': 0.2,
            'style_embedding': np.zeros(128).tolist()
        }
    
    def _build_relationship_map(self) -> Dict[str, Dict[str, Any]]:
        """æ„å»ºå…³ç³»æ˜ å°„"""
        cursor = self.db_conn.cursor()
        
        cursor.execute("""
            SELECT 
                target_person,
                COUNT(*) as interaction_count,
                AVG(COALESCE(emotional_context, 0)) as avg_emotion
            FROM personality_training_samples
            WHERE user_id = ? AND target_person IS NOT NULL
            GROUP BY target_person
        """, (self.config.user_id,))
        
        relationship_map = {}
        for row in cursor.fetchall():
            person = row[0]
            relationship_map[person] = {
                'interaction_count': row[1],
                'intimacy_level': min(row[1] / 100, 1.0),
                'avg_emotion': row[2] if row[2] else 0.0,
                # ç®€åŒ–çš„å…³ç³»embedding
                'relation_embedding': np.random.randn(64).tolist()  # å ä½ç¬¦
            }
        
        return relationship_map
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹ (ä¼ªä»£ç )"""
        # å®é™…å®ç°åº”åŠ è½½çœŸå®çš„Geminiæ¨¡å‹æˆ–å…¶ä»–LLM
        # è¿™é‡Œè¿”å›ä¸€ä¸ªå ä½ç¬¦
        logger.info("   ğŸ“¦ Model initialization (placeholder)")
        
        return {
            'base_model': self.config.base_model,
            'lora_config': {
                'rank': self.config.lora_rank,
                'alpha': self.config.lora_alpha
            },
            'initialized_at': datetime.now().isoformat()
        }
    
    def _train_epoch(
        self, 
        model, 
        train_data: List[Dict], 
        style_profile: Dict,
        relationship_map: Dict
    ) -> float:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        total_loss = 0.0
        num_batches = len(train_data) // self.config.batch_size
        
        for batch_idx in range(num_batches):
            # è·å–batchæ•°æ®
            batch_start = batch_idx * self.config.batch_size
            batch_end = batch_start + self.config.batch_size
            batch_samples = train_data[batch_start:batch_end]
            
            # å‡†å¤‡batch
            batch = self._prepare_batch(batch_samples, style_profile, relationship_map)
            
            # è®¡ç®—å¤šä»»åŠ¡æŸå¤±
            losses = self._compute_multitask_loss(model, batch, style_profile, relationship_map)
            
            # æ€»æŸå¤± (åŠ æƒå’Œ)
            total_batch_loss = (
                self.config.gen_loss_weight * losses['gen'] +
                self.config.style_loss_weight * losses['style'] +
                self.config.relation_loss_weight * losses['relation'] +
                self.config.contrastive_loss_weight * losses['contrastive']
            )
            
            total_loss += total_batch_loss
            
            # è®°å½•æŸå¤±
            if batch_idx % self.config.logging_steps == 0:
                logger.info(
                    f"   Step {batch_idx}/{num_batches} - "
                    f"Loss: {total_batch_loss:.4f} "
                    f"(Gen: {losses['gen']:.4f}, Style: {losses['style']:.4f}, "
                    f"Rel: {losses['relation']:.4f}, Contra: {losses['contrastive']:.4f})"
                )
                
                # ä¿å­˜åˆ°ç»Ÿè®¡
                for key, value in losses.items():
                    self.training_stats['losses'][key].append(value)
                self.training_stats['losses']['total'].append(total_batch_loss)
            
            self.training_stats['total_steps'] += 1
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def _validate_epoch(
        self, 
        model, 
        val_data: List[Dict],
        style_profile: Dict,
        relationship_map: Dict
    ) -> float:
        """éªŒè¯ä¸€ä¸ªepoch"""
        total_loss = 0.0
        num_batches = len(val_data) // self.config.batch_size
        
        for batch_idx in range(num_batches):
            batch_start = batch_idx * self.config.batch_size
            batch_end = batch_start + self.config.batch_size
            batch_samples = val_data[batch_start:batch_end]
            
            batch = self._prepare_batch(batch_samples, style_profile, relationship_map)
            losses = self._compute_multitask_loss(model, batch, style_profile, relationship_map)
            
            total_batch_loss = (
                self.config.gen_loss_weight * losses['gen'] +
                self.config.style_loss_weight * losses['style'] +
                self.config.relation_loss_weight * losses['relation'] +
                self.config.contrastive_loss_weight * losses['contrastive']
            )
            
            total_loss += total_batch_loss
        
        avg_loss = total_loss / num_batches
        return avg_loss
    
    def _prepare_batch(
        self, 
        samples: List[Dict],
        style_profile: Dict,
        relationship_map: Dict
    ) -> TrainingBatch:
        """å‡†å¤‡è®­ç»ƒbatch"""
        # ç®€åŒ–ç‰ˆå®ç° (å®é™…éœ€è¦tokenizationç­‰)
        batch_size = len(samples)
        
        # æ„å»ºè¾“å…¥åºåˆ— (å ä½ç¬¦)
        input_ids = np.random.randint(0, 1000, (batch_size, 512))
        attention_mask = np.ones((batch_size, 512))
        labels = np.random.randint(0, 1000, (batch_size, 512))
        
        # é£æ ¼embedding (æ‰€æœ‰æ ·æœ¬å…±äº«ç”¨æˆ·é£æ ¼)
        style_embeddings = np.array([style_profile['style_embedding']] * batch_size)
        
        # å…³ç³»context (æ ¹æ®target_personæå–)
        relationship_context = []
        for sample in samples:
            person = sample.get('target_person')
            if person and person in relationship_map:
                rel_emb = relationship_map[person]['relation_embedding']
            else:
                rel_emb = np.zeros(64).tolist()  # é»˜è®¤
            relationship_context.append(rel_emb)
        relationship_context = np.array(relationship_context)
        
        # è´Ÿæ ·æœ¬æ ‡è®°
        is_negative = np.array([
            sample.get('augmentation_metadata', {}).get('is_negative', False)
            for sample in samples
        ], dtype=bool)
        
        return TrainingBatch(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels,
            style_embeddings=style_embeddings,
            relationship_context=relationship_context,
            is_negative=is_negative,
            sample_ids=[s['id'] for s in samples],
            target_persons=[s.get('target_person') for s in samples]
        )
    
    def _compute_multitask_loss(
        self,
        model,
        batch: TrainingBatch,
        style_profile: Dict,
        relationship_map: Dict
    ) -> Dict[str, float]:
        """è®¡ç®—å¤šä»»åŠ¡æŸå¤±"""
        
        # 1. Generation Loss (æ ‡å‡†ç”ŸæˆæŸå¤±)
        gen_loss = self._compute_generation_loss(model, batch)
        
        # 2. Style Consistency Loss (é£æ ¼ä¸€è‡´æ€§)
        style_loss = self._compute_style_loss(model, batch, style_profile)
        
        # 3. Relationship-Aware Loss (å…³ç³»é€‚é…)
        relation_loss = self._compute_relationship_loss(model, batch, relationship_map)
        
        # 4. Contrastive Loss (å¯¹æ¯”å­¦ä¹ )
        contrastive_loss = self._compute_contrastive_loss(model, batch)
        
        return {
            'gen': gen_loss,
            'style': style_loss,
            'relation': relation_loss,
            'contrastive': contrastive_loss
        }
    
    def _compute_generation_loss(self, model, batch: TrainingBatch) -> float:
        """è®¡ç®—ç”ŸæˆæŸå¤± (Cross-Entropy)"""
        # ä¼ªä»£ç : å®é™…åº”è°ƒç”¨æ¨¡å‹çš„forward pass
        # loss = F.cross_entropy(logits, batch.labels)
        
        # å ä½ç¬¦: éšæœºæŸå¤±
        return np.random.uniform(0.5, 2.0)
    
    def _compute_style_loss(
        self, 
        model, 
        batch: TrainingBatch,
        style_profile: Dict
    ) -> float:
        """è®¡ç®—é£æ ¼ä¸€è‡´æ€§æŸå¤± (Cosine Distance)"""
        # ä¼ªä»£ç :
        # 1. ä»æ¨¡å‹è¾“å‡ºä¸­æå–ç”Ÿæˆæ–‡æœ¬çš„é£æ ¼embedding
        # 2. ä¸ç”¨æˆ·é£æ ¼profileçš„embeddingè®¡ç®—ä½™å¼¦è·ç¦»
        # style_loss = 1 - cosine_similarity(generated_style_emb, user_style_emb)
        
        # å ä½ç¬¦
        return np.random.uniform(0.1, 0.5)
    
    def _compute_relationship_loss(
        self,
        model,
        batch: TrainingBatch,
        relationship_map: Dict
    ) -> float:
        """è®¡ç®—å…³ç³»é€‚é…æŸå¤± (KL Divergence)"""
        # ä¼ªä»£ç :
        # 1. æ ¹æ®target_personè·å–æœŸæœ›çš„å…³ç³»contextåˆ†å¸ƒ
        # 2. ä»æ¨¡å‹è¾“å‡ºä¸­æå–å®é™…ç”Ÿæˆçš„å…³ç³»context
        # 3. è®¡ç®—KLæ•£åº¦
        # relation_loss = kl_divergence(expected_rel_dist, generated_rel_dist)
        
        # å ä½ç¬¦
        return np.random.uniform(0.05, 0.3)
    
    def _compute_contrastive_loss(self, model, batch: TrainingBatch) -> float:
        """è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤± (InfoNCE)"""
        # ä¼ªä»£ç :
        # 1. å¯¹äºæ¯ä¸ªanchoræ ·æœ¬,æ‰¾åˆ°positive (çœŸå®ç”¨æˆ·å›å¤) å’Œ negative samples
        # 2. è®¡ç®—embeddingè·ç¦»
        # 3. InfoNCE: -log(exp(sim(anchor, pos)) / (exp(sim(anchor, pos)) + sum(exp(sim(anchor, neg)))))
        
        # ä»…å¯¹æ ‡è®°ä¸ºè´Ÿæ ·æœ¬çš„batchè®¡ç®—
        if not batch.is_negative.any():
            return 0.0
        
        # å ä½ç¬¦
        return np.random.uniform(0.05, 0.2)
    
    def _save_checkpoint(self, model, epoch: int, val_loss: float) -> str:
        """ä¿å­˜checkpoint"""
        checkpoint_dir = os.path.join(
            self.config.output_dir,
            self.config.user_id,
            f"checkpoint-epoch{epoch}"
        )
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        checkpoint_path = os.path.join(checkpoint_dir, "model.json")
        
        # ä¿å­˜æ¨¡å‹ (ä¼ªä»£ç )
        with open(checkpoint_path, 'w') as f:
            json.dump({
                'model': model,
                'epoch': epoch,
                'val_loss': val_loss,
                'config': self.config.__dict__
            }, f, indent=2)
        
        return checkpoint_path
    
    def _save_final_model(self, best_model_path: Optional[str]) -> Dict[str, Any]:
        """ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œå…ƒæ•°æ®"""
        timestamp = int(datetime.now().timestamp() * 1000)
        final_dir = os.path.join(
            self.config.output_dir,
            self.config.user_id,
            f"final-{timestamp}"
        )
        os.makedirs(final_dir, exist_ok=True)
        
        # å¤åˆ¶æœ€ä½³æ¨¡å‹
        if best_model_path:
            import shutil
            shutil.copy(best_model_path, os.path.join(final_dir, "model.json"))
        
        # ä¿å­˜è®­ç»ƒå…ƒæ•°æ®
        metadata = {
            'user_id': self.config.user_id,
            'model_version': f"v{timestamp}",
            'training_config': self.config.__dict__,
            'training_stats': self.training_stats,
            'best_val_loss': self.training_stats['best_val_loss'],
            'total_steps': self.training_stats['total_steps'],
            'model_path': final_dir,
            'trained_at': datetime.now().isoformat()
        }
        
        metadata_path = os.path.join(final_dir, "metadata.json")
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # ä¿å­˜æŸå¤±æ›²çº¿
        losses_path = os.path.join(final_dir, "training_losses.json")
        with open(losses_path, 'w') as f:
            json.dump(self.training_stats['losses'], f, indent=2)
        
        logger.info(f"   ğŸ’¾ Saved final model to: {final_dir}")
        logger.info(f"   ğŸ“Š Saved metadata to: {metadata_path}")
        
        # å†™å…¥æ•°æ®åº“
        self._record_trained_model(metadata)
        
        return metadata
    
    def _record_trained_model(self, metadata: Dict[str, Any]):
        """è®°å½•è®­ç»ƒå¥½çš„æ¨¡å‹åˆ°æ•°æ®åº“"""
        cursor = self.db_conn.cursor()
        
        cursor.execute("""
            INSERT INTO personality_models 
            (user_id, model_version, model_type, model_path, 
             training_samples_count, training_loss, hyperparameters, 
             is_active, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, 1, ?)
        """, (
            self.config.user_id,
            metadata['model_version'],
            self.config.model_type,
            metadata['model_path'],
            metadata['training_stats']['total_steps'] * self.config.batch_size,
            metadata['best_val_loss'],
            json.dumps(self.config.__dict__),
            int(datetime.now().timestamp() * 1000)
        ))
        
        self.db_conn.commit()
        logger.info("   âœ“ Recorded model in database")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¤šä»»åŠ¡äººæ ¼è®­ç»ƒå™¨')
    
    # åŸºç¡€å‚æ•°
    parser.add_argument('--user-id', type=str, required=True, help='ç”¨æˆ·ID')
    parser.add_argument('--db-path', type=str, default='./self_agent.db', help='æ•°æ®åº“è·¯å¾„')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--base-model', type=str, default='gemini-2.0-flash-exp', help='åŸºç¡€æ¨¡å‹')
    parser.add_argument('--lora-rank', type=int, default=16, help='LoRA rank')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=3, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=4, help='æ‰¹å¤§å°')
    parser.add_argument('--learning-rate', type=float, default=2e-5, help='å­¦ä¹ ç‡')
    
    # æŸå¤±æƒé‡
    parser.add_argument('--gen-loss-weight', type=float, default=1.0, help='ç”ŸæˆæŸå¤±æƒé‡')
    parser.add_argument('--style-loss-weight', type=float, default=0.3, help='é£æ ¼æŸå¤±æƒé‡')
    parser.add_argument('--relation-loss-weight', type=float, default=0.2, help='å…³ç³»æŸå¤±æƒé‡')
    parser.add_argument('--contrastive-loss-weight', type=float, default=0.1, help='å¯¹æ¯”æŸå¤±æƒé‡')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--use-augmented', action='store_true', help='ä½¿ç”¨å¢å¼ºæ•°æ®')
    parser.add_argument('--min-samples', type=int, default=50, help='æœ€å°æ ·æœ¬æ•°')
    
    # è¾“å‡ºå‚æ•°
    parser.add_argument('--output-dir', type=str, default='./models', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # æ„å»ºé…ç½®
    config = TrainingConfig(
        user_id=args.user_id,
        db_path=args.db_path,
        base_model=args.base_model,
        lora_rank=args.lora_rank,
        epochs=args.epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        gen_loss_weight=args.gen_loss_weight,
        style_loss_weight=args.style_loss_weight,
        relation_loss_weight=args.relation_loss_weight,
        contrastive_loss_weight=args.contrastive_loss_weight,
        use_augmented_data=args.use_augmented,
        min_samples=args.min_samples,
        output_dir=args.output_dir
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶è®­ç»ƒ
    trainer = MultiTaskPersonaTrainer(config)
    result = trainer.train()
    
    print("\n" + "="*80)
    print("ğŸ‰ Training Result:")
    print(json.dumps(result, indent=2))
    print("="*80 + "\n")


if __name__ == '__main__':
    main()
